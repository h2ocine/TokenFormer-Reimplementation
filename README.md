# TokenFormer - RÃ©implÃ©mentation

Ce projet vise Ã  rÃ©implÃ©menter le papier **"TOKENFORMER: RETHINKING TRANSFORMER SCALING WITH TOKENIZED MODEL PARAMETERS"**. Il explore une approche alternative au scaling des Transformers en optimisant la reprÃ©sentation des paramÃ¨tres du modÃ¨le sous forme tokenisÃ©e.

## ğŸ“Œ Contenu du projet

### ğŸ“‚ `models/`
Ce dossier contient l'implÃ©mentation des architectures utilisÃ©es dans le projet.  
- `tokenformer.py` : ImplÃ©mentation du modÃ¨le TokenFormer.
- `transformer.py` : ImplÃ©mentation d'un Transformer standard.
- `self_attention.py` : Module de self-attention pour le TokenFormer.
- `pattention.py` : Module de Pattention (une variante de l'attention).

### ğŸ“‚ `data/`
Ce dossier est dÃ©diÃ© Ã  la gestion des donnÃ©es.  
- `dataset.py` : DÃ©finit les fonctions de chargement et de prÃ©traitement des donnÃ©es.

### ğŸ“‚ `training/`
Ce dossier regroupe les fonctions liÃ©s Ã  l'entraÃ®nement des modÃ¨les.  
- `train.py` : Contient les fonctions pour :
  - EntraÃ®ner un modÃ¨le Transformer standard.
  - EntraÃ®ner un modÃ¨le TokenFormer.
  - EntraÃ®ner un modÃ¨le avec **scaling progressif**.

### ğŸ“‚ `checkpoints/`
RÃ©pertoire dÃ©diÃ© Ã  la sauvegarde des modÃ¨les entraÃ®nÃ©s sous forme de checkpoints.

### ğŸ“‚ `utils/`
Contient d'autres expÃ©rimentations altÃ©rnatives.

### ğŸ“‚ `other/`
Ce dossier contient des expÃ©rimentations et des versions intermÃ©diaires du projet. Il inclut :
- `tests_version0/` : Tests initiaux du modÃ¨le.
- `training_version0/` : Versions de test des scripts d'entraÃ®nement.
- `models_version0/` : Versions prÃ©liminaires des modÃ¨les.

## ğŸ‘¨â€ğŸ’» Auteurs
Ce projet a Ã©tÃ© rÃ©alisÃ© par :
- **Hocine Kadem**
- **Huang Tian**
- **Kande Seydina**

ğŸ“ **Sorbonne UniversitÃ©**

## ğŸ“œ RÃ©fÃ©rences
- **TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters**  
  _[https://arxiv.org/abs/2410.23168]_

