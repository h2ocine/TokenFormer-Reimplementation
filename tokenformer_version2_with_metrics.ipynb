{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "        # to obtain logits (before softmax) for the vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        # Linear layer for logits\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class remains unchanged\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab_size, max_seq_len):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.vocab = {word: idx for idx, word in enumerate(set(self.text))}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass through the model.\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "            y_batch = y_batch.view(-1)  # Flatten targets to shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss for the current batch.\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)  # Multiply by the number of tokens.\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    # Calculate the average loss per token.\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # Compute the perplexity by exponentiating the average loss.\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    # Configurations\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    vocab_size=10000    # Valeur temporaire, sera mise à jour avec vocab_size réel\n",
    "    hidden_dim=32\n",
    "    num_heads=4\n",
    "    #num_layers=2,\n",
    "    #num_tokens=10,\n",
    "    max_seq_len=50\n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = PokemonDataset(file_path='training/pokemon.txt', vocab_size=vocab_size, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size  # Mettre à jour vocab_size après avoir créé le vocabulaire\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"debut de l'entrainement\")\n",
    "    start_time = time.time()\n",
    "    num_epochs=10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            device = next(model.parameters()).device\n",
    "\n",
    "            # Déplacer les données sur le même appareil que le modèle (GPU ou CPU)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            #print(\"outputs:\", outputs.shape)\n",
    "\n",
    "            # Aplatir les logits pour la CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            #print(\"outputs aplati:\", outputs.shape)\n",
    "\n",
    "            # Aplatir les labels\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if use_metrics:\n",
    "            # Afficher la perte moyenne de train et test, aussi perplexity par epoch\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss / len(dataloader):.4f}, Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss / len(dataloader):.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debut de l'entrainement\n",
      "Epoch 1/10, Train Loss: 5.7301, Eval Loss: 4.6361, Perplexity: 103.1455\n",
      "Epoch 2/10, Train Loss: 4.3432, Eval Loss: 3.9347, Perplexity: 51.1473\n",
      "Epoch 3/10, Train Loss: 3.8571, Eval Loss: 3.5632, Perplexity: 35.2756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(use_metrics)\u001b[0m\n\u001b[1;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     84\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 85\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (use_metrics):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Afficher la perte moyenne de train et test, aussi perplexity par epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    344\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 345\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    348\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(use_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "\n",
    "    def add_new_params(self, num_new_tokens):\n",
    "        device = self.key_param_tokens.device  # Get the current device\n",
    "\n",
    "        new_key_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_key_dim), device=device))\n",
    "        new_value_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_value_dim), device=device))\n",
    "\n",
    "        # Concatenate with existing parameters\n",
    "        self.key_param_tokens = nn.Parameter(torch.cat([self.key_param_tokens, new_key_tokens], dim=0))\n",
    "        self.value_param_tokens = nn.Parameter(torch.cat([self.value_param_tokens, new_value_tokens], dim=0))\n",
    "\n",
    "        #print(f\"Added {num_new_tokens} new tokens. Total tokens now: {self.key_param_tokens.shape[0]}\")\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "def progressive_train_and_scale(\n",
    "    file_path,\n",
    "    vocab_size,\n",
    "    max_seq_len,\n",
    "    hidden_dim,\n",
    "    num_heads,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    scaling_steps,\n",
    "    new_tokens_per_step,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='checkpoints/tokenformer_base.pth',\n",
    "    use_metrics=False,\n",
    "):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = PokemonDataset(file_path=file_path, vocab_size=vocab_size, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate the model and move to device\n",
    "    model = TokenformerLayer(hidden_size=hidden_dim, vocab_size=vocab_size, num_attention_heads=num_heads, max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    initial_start_time = time.time()\n",
    "    # Initial Training and Saving\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch).view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if use_metrics:\n",
    "            avg_epoch_loss = total_loss / len(dataloader)\n",
    "            # compute perplexity and evaluation loss\n",
    "            eval_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "        else:\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "    \n",
    "    initial_end_time = time.time()\n",
    "    print(f\"Initial training time: {initial_end_time - initial_start_time:.2f} seconds\")\n",
    "\n",
    "    # Save the initial model\n",
    "    save_model(model, checkpoint_path)\n",
    "    print(f\"Initial model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    # Load model before scaling\n",
    "    load_model(model, checkpoint_path)\n",
    "    fine_tuning_start_time = time.time()\n",
    "    # Progressive scaling and fine-tuning\n",
    "    for step in range(scaling_steps):\n",
    "        # Add new parameters\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, Pattention):\n",
    "                module.add_new_params(new_tokens_per_step)\n",
    "\n",
    "        # Save the scaled model checkpoint\n",
    "        checkpoint_path_step = f'checkpoints/tokenformer_scaled_step_{step + 1}.pth'\n",
    "        save_model(model, checkpoint_path_step)\n",
    "\n",
    "        print(f\"Step {step + 1}: Model parameters after scaling: {count_parameters(model)}\")\n",
    "\n",
    "        # Load the latest checkpoint before fine-tuning\n",
    "        load_model(model, checkpoint_path_step)\n",
    "\n",
    "        # Fine-tuning for the current step\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).view(-1, vocab_size)\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if use_metrics:\n",
    "                avg_epoch_loss = total_loss / len(dataloader)\n",
    "                # compute perplexity and evaluation loss\n",
    "                eval_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                    f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "            else:\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "            \n",
    "    fine_tuning_end_time = time.time()\n",
    "    print(f\"Progressive scaling and fine-tuning time: {fine_tuning_end_time - fine_tuning_start_time:.2f} seconds\")\n",
    "    print(f\"Total training time: {fine_tuning_end_time - initial_start_time:.2f} seconds\")\n",
    "    # Save the final scaled model checkpoint\n",
    "    final_checkpoint_path = 'checkpoints/tokenformer_final_scaled.pth'\n",
    "    save_model(model, final_checkpoint_path)\n",
    "    print(f\"Final scaled model saved to {final_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial training...\n",
      "Initial Training Epoch 1/5, Loss: 0.00405199784068704\n",
      "Initial Training Epoch 2/5, Loss: 0.0010964027286532532\n",
      "Initial Training Epoch 3/5, Loss: 0.00039806937101885217\n",
      "Initial Training Epoch 4/5, Loss: 0.00019192799218113225\n",
      "Initial Training Epoch 5/5, Loss: 0.00011736223036049437\n",
      "Model saved to checkpoints/tokenformer_base.pth\n",
      "Initial model parameters: 345888\n",
      "Model loaded from checkpoints/tokenformer_base.pth\n",
      "Model saved to checkpoints/tokenformer_scaled_step_1.pth\n",
      "Step 1: Model parameters after scaling: 347488\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/tk1z4ld57254_sgr1npxv2qm0000gn/T/ipykernel_25417/2821629227.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3, Fine-tune Epoch 1/5, Loss: 6.270982708548387e-05\n",
      "Step 1/3, Fine-tune Epoch 2/5, Loss: 4.9207377172659395e-05\n",
      "Step 1/3, Fine-tune Epoch 3/5, Loss: 4.154280187602042e-05\n",
      "Step 1/3, Fine-tune Epoch 4/5, Loss: 5.210743198133251e-05\n",
      "Step 1/3, Fine-tune Epoch 5/5, Loss: 4.2491013088074234e-05\n",
      "Model saved to checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2: Model parameters after scaling: 349088\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2/3, Fine-tune Epoch 1/5, Loss: 3.3079449172570354e-05\n",
      "Step 2/3, Fine-tune Epoch 2/5, Loss: 2.6569022649267098e-05\n"
     ]
    }
   ],
   "source": [
    "progressive_train_and_scale(\n",
    "    file_path='training/pokemon.txt',\n",
    "    vocab_size=10000,\n",
    "    max_seq_len=50,\n",
    "    hidden_dim=32,\n",
    "    num_heads=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    scaling_steps=3,\n",
    "    new_tokens_per_step=5,\n",
    "    use_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
