{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1,token_num=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        token_num=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "            token_num=token_num\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "        # to obtain logits (before softmax) for the vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        # Linear layer for logits\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class remains unchanged\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, max_seq_len):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        \n",
    "        vocab = sorted(set(self.text))\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass through the model.\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "            y_batch = y_batch.view(-1)  # Flatten targets to shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss for the current batch.\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)  # Multiply by the number of tokens.\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    # Calculate the average loss per token.\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # Compute the perplexity by exponentiating the average loss.\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    # Configurations\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # use M1 GPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # use CPU if no M1 GPU\n",
    "    hidden_dim=32 \n",
    "    num_heads=4\n",
    "    max_seq_len=32\n",
    "    batch_size=32\n",
    "    num_epochs=10\n",
    "    learning_rate=0.001\n",
    "    token_num=32\n",
    "    # vocab_size=10000\n",
    "    #num_layers=2,\n",
    "    #num_tokens=10,\n",
    "   \n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = PokemonDataset(file_path='training/pokemon.txt', max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size  # Mettre à jour vocab_size après avoir créé le vocabulaire\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len,token_num=token_num)\n",
    "    model = model.to(device)\n",
    "    print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"number of training samples:\", len(dataset))\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"debut de l'entrainement\")\n",
    "    total_training_time = 0.0  # record the total training time\n",
    "    overall_start_time = time.time()  # record the overall start time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # record the start time of the current epoch\n",
    "        epoch_train_start = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les logits pour la CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les labels\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # record the end time of the current epoch\n",
    "        epoch_training_time = time.time() - epoch_train_start\n",
    "        total_training_time += epoch_training_time\n",
    "\n",
    "        if use_metrics:\n",
    "            # record the start time of the evaluation\n",
    "            eval_start = time.time()\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            eval_time = time.time() - eval_start\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s, Eval Time: {eval_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s\")\n",
    "\n",
    "    print(f\"Total training time (excluding evaluation): {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 10655\n",
      "model parameters: 703967\n",
      "number of training samples: 89205\n",
      "Total training tokens: 27.65M\n",
      "debut de l'entrainement\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 98\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(use_metrics)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Backpropagation et optimisation\u001b[39;00m\n\u001b[1;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    101\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(use_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "\n",
    "    def add_new_params(self, num_new_tokens):\n",
    "        device = self.key_param_tokens.device  # Get the current device\n",
    "\n",
    "        new_key_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_key_dim), device=device))\n",
    "        new_value_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_value_dim), device=device))\n",
    "\n",
    "        # Concatenate with existing parameters\n",
    "        self.key_param_tokens = nn.Parameter(torch.cat([self.key_param_tokens, new_key_tokens], dim=0))\n",
    "        self.value_param_tokens = nn.Parameter(torch.cat([self.value_param_tokens, new_value_tokens], dim=0))\n",
    "\n",
    "        #print(f\"Added {num_new_tokens} new tokens. Total tokens now: {self.key_param_tokens.shape[0]}\")\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def progressive_train_and_scale(\n",
    "    file_path,\n",
    "    vocab_size,\n",
    "    max_seq_len,\n",
    "    hidden_dim,\n",
    "    num_heads,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    scaling_steps,\n",
    "    new_tokens_per_step,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='checkpoints/tokenformer_base.pth',\n",
    "    use_metrics=False,\n",
    "    test_size=0.1,  # Default 10% of data as test set\n",
    "):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = PokemonDataset(file_path=file_path, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    \n",
    "    # Train-test split\n",
    "    train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model and move to device\n",
    "    model = TokenformerLayer(hidden_size=hidden_dim, vocab_size=vocab_size, num_attention_heads=num_heads, max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "    print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"number of training samples:\", len(dataset))\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs * (scaling_steps + 1)\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    initial_start_time = time.time()\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch).view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if use_metrics:\n",
    "            avg_epoch_loss = total_loss / len(train_loader)\n",
    "            eval_loss, perplexity = estimate_perplexity(model, val_loader, criterion, device)\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "        else:\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    initial_end_time = time.time()\n",
    "    print(f\"Initial training time: {initial_end_time - initial_start_time:.2f} seconds\")\n",
    "    save_model(model, checkpoint_path)\n",
    "    print(f\"Initial model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    load_model(model, checkpoint_path)\n",
    "    fine_tuning_start_time = time.time()\n",
    "    for step in range(scaling_steps):\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, Pattention):\n",
    "                module.add_new_params(new_tokens_per_step)\n",
    "        print(f\"Step {step + 1}: Model parameters after adding new params: {count_parameters(model)}\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        checkpoint_path_step = f'checkpoints/tokenformer_scaled_step_{step + 1}.pth'\n",
    "        save_model(model, checkpoint_path_step)\n",
    "        print(f\"Step {step + 1}: Model parameters after scaling: {count_parameters(model)}\")\n",
    "        load_model(model, checkpoint_path_step)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).view(-1, vocab_size)\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if use_metrics:\n",
    "                avg_epoch_loss = total_loss / len(train_loader)\n",
    "                eval_loss, perplexity = estimate_perplexity(model, val_loader, criterion, device)\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                    f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "            else:\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "            \n",
    "    fine_tuning_end_time = time.time()\n",
    "    print(f\"Progressive scaling and fine-tuning time: {fine_tuning_end_time - fine_tuning_start_time:.2f} seconds\")\n",
    "    print(f\"Total training time: {fine_tuning_end_time - initial_start_time:.2f} seconds\")\n",
    "    final_checkpoint_path = 'checkpoints/tokenformer_final_scaled.pth'\n",
    "    save_model(model, final_checkpoint_path)\n",
    "    print(f\"Final scaled model saved to {final_checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 697503\n",
      "number of training samples: 89187\n",
      "Total training tokens: 87.40M\n",
      "input_layernorm.weight: 32 parameters\n",
      "input_layernorm.bias: 32 parameters\n",
      "post_attention_layernorm.weight: 32 parameters\n",
      "post_attention_layernorm.bias: 32 parameters\n",
      "token_embedding.weight: 340960 parameters\n",
      "position_embedding.weight: 1600 parameters\n",
      "attention.query.key_param_tokens: 320 parameters\n",
      "attention.query.value_param_tokens: 320 parameters\n",
      "attention.key.key_param_tokens: 320 parameters\n",
      "attention.key.value_param_tokens: 320 parameters\n",
      "attention.value.key_param_tokens: 320 parameters\n",
      "attention.value.value_param_tokens: 320 parameters\n",
      "attention.out_proj.key_param_tokens: 320 parameters\n",
      "attention.out_proj.value_param_tokens: 320 parameters\n",
      "mlp.key_param_tokens: 320 parameters\n",
      "mlp.value_param_tokens: 320 parameters\n",
      "lm_head.weight: 340960 parameters\n",
      "lm_head.bias: 10655 parameters\n",
      "Starting initial training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprogressive_train_and_scale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining/pokemon.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_tokens_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 75\u001b[0m, in \u001b[0;36mprogressive_train_and_scale\u001b[0;34m(file_path, vocab_size, max_seq_len, hidden_dim, num_heads, batch_size, num_epochs, scaling_steps, new_tokens_per_step, learning_rate, checkpoint_path, use_metrics, test_size)\u001b[0m\n\u001b[1;32m     73\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     74\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 75\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_metrics:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    344\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 345\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    348\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progressive_train_and_scale(\n",
    "    file_path='training/pokemon.txt',\n",
    "    vocab_size=10000,\n",
    "    max_seq_len=50,\n",
    "    hidden_dim=32,\n",
    "    num_heads=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    scaling_steps=3,\n",
    "    new_tokens_per_step=8,\n",
    "    use_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
