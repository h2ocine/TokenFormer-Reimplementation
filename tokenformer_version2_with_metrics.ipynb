{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1,token_num=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        token_num=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "            token_num=token_num\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "        # to obtain logits (before softmax) for the vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        # Linear layer for logits\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class remains unchanged\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, max_seq_len):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        \n",
    "        vocab = sorted(set(self.text))\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass through the model.\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "            y_batch = y_batch.view(-1)  # Flatten targets to shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss for the current batch.\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)  # Multiply by the number of tokens.\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    # Calculate the average loss per token.\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # Compute the perplexity by exponentiating the average loss.\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    # Configurations\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # use M1 GPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # use CPU if no M1 GPU\n",
    "    hidden_dim=32 #same as embedding dim\n",
    "    num_heads=4\n",
    "    max_seq_len=32\n",
    "    batch_size=32\n",
    "    num_epochs=10\n",
    "    learning_rate=0.001\n",
    "    token_num=32\n",
    "    # vocab_size=10000\n",
    "    #num_layers=2,\n",
    "    #num_tokens=10,\n",
    "   \n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = PokemonDataset(file_path='training/pokemon.txt', max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size  # Mettre à jour vocab_size après avoir créé le vocabulaire\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len,token_num=token_num)\n",
    "    model = model.to(device)\n",
    "    print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"number of training samples:\", len(dataset))\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"debut de l'entrainement\")\n",
    "    total_training_time = 0.0  # record the total training time\n",
    "    overall_start_time = time.time()  # record the overall start time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # record the start time of the current epoch\n",
    "        epoch_train_start = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les logits pour la CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les labels\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # record the end time of the current epoch\n",
    "        epoch_training_time = time.time() - epoch_train_start\n",
    "        total_training_time += epoch_training_time\n",
    "\n",
    "        if use_metrics:\n",
    "            # record the start time of the evaluation\n",
    "            eval_start = time.time()\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            eval_time = time.time() - eval_start\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s, Eval Time: {eval_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s\")\n",
    "\n",
    "    print(f\"Total training time (excluding evaluation): {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 10655\n",
      "model parameters: 703967\n",
      "number of training samples: 89205\n",
      "Total training tokens: 27.65M\n",
      "debut de l'entrainement\n"
     ]
    }
   ],
   "source": [
    "train(use_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "\n",
    "    def add_new_params(self, num_new_tokens):\n",
    "        device = self.key_param_tokens.device  # Get the current device\n",
    "\n",
    "        new_key_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_key_dim), device=device))\n",
    "        new_value_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_value_dim), device=device))\n",
    "\n",
    "        # Concatenate with existing parameters\n",
    "        self.key_param_tokens = nn.Parameter(torch.cat([self.key_param_tokens, new_key_tokens], dim=0))\n",
    "        self.value_param_tokens = nn.Parameter(torch.cat([self.value_param_tokens, new_value_tokens], dim=0))\n",
    "\n",
    "        #print(f\"Added {num_new_tokens} new tokens. Total tokens now: {self.key_param_tokens.shape[0]}\")\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "def progressive_train_and_scale(\n",
    "    file_path,\n",
    "    vocab_size,\n",
    "    max_seq_len,\n",
    "    hidden_dim,\n",
    "    num_heads,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    scaling_steps,\n",
    "    new_tokens_per_step,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='checkpoints/tokenformer_base.pth',\n",
    "    use_metrics=False,\n",
    "):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = PokemonDataset(file_path=file_path, vocab_size=vocab_size, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate the model and move to device\n",
    "    model = TokenformerLayer(hidden_size=hidden_dim, vocab_size=vocab_size, num_attention_heads=num_heads, max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    initial_start_time = time.time()\n",
    "    # Initial Training and Saving\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch).view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if use_metrics:\n",
    "            avg_epoch_loss = total_loss / len(dataloader)\n",
    "            # compute perplexity and evaluation loss\n",
    "            eval_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "        else:\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "    \n",
    "    initial_end_time = time.time()\n",
    "    print(f\"Initial training time: {initial_end_time - initial_start_time:.2f} seconds\")\n",
    "\n",
    "    # Save the initial model\n",
    "    save_model(model, checkpoint_path)\n",
    "    print(f\"Initial model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    # Load model before scaling\n",
    "    load_model(model, checkpoint_path)\n",
    "    fine_tuning_start_time = time.time()\n",
    "    # Progressive scaling and fine-tuning\n",
    "    for step in range(scaling_steps):\n",
    "        # Add new parameters\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, Pattention):\n",
    "                module.add_new_params(new_tokens_per_step)\n",
    "\n",
    "        # Save the scaled model checkpoint\n",
    "        checkpoint_path_step = f'checkpoints/tokenformer_scaled_step_{step + 1}.pth'\n",
    "        save_model(model, checkpoint_path_step)\n",
    "\n",
    "        print(f\"Step {step + 1}: Model parameters after scaling: {count_parameters(model)}\")\n",
    "\n",
    "        # Load the latest checkpoint before fine-tuning\n",
    "        load_model(model, checkpoint_path_step)\n",
    "\n",
    "        # Fine-tuning for the current step\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).view(-1, vocab_size)\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if use_metrics:\n",
    "                avg_epoch_loss = total_loss / len(dataloader)\n",
    "                # compute perplexity and evaluation loss\n",
    "                eval_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                    f\"Train Loss: {avg_epoch_loss:.4f}, Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "            else:\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "            \n",
    "    fine_tuning_end_time = time.time()\n",
    "    print(f\"Progressive scaling and fine-tuning time: {fine_tuning_end_time - fine_tuning_start_time:.2f} seconds\")\n",
    "    print(f\"Total training time: {fine_tuning_end_time - initial_start_time:.2f} seconds\")\n",
    "    # Save the final scaled model checkpoint\n",
    "    final_checkpoint_path = 'checkpoints/tokenformer_final_scaled.pth'\n",
    "    save_model(model, final_checkpoint_path)\n",
    "    print(f\"Final scaled model saved to {final_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial training...\n",
      "Initial Training Epoch 1/5, Train Loss: 5.7068, Eval Loss: 4.6041, Perplexity: 99.8899\n",
      "Initial Training Epoch 2/5, Train Loss: 4.3234, Eval Loss: 3.9233, Perplexity: 50.5690\n",
      "Initial Training Epoch 3/5, Train Loss: 3.8472, Eval Loss: 3.5622, Perplexity: 35.2416\n",
      "Initial Training Epoch 4/5, Train Loss: 3.5602, Eval Loss: 3.3242, Perplexity: 27.7777\n",
      "Initial Training Epoch 5/5, Train Loss: 3.3608, Eval Loss: 3.1454, Perplexity: 23.2281\n",
      "Initial training time: 592.59 seconds\n",
      "Model saved to checkpoints/tokenformer_base.pth\n",
      "Initial model parameters: 697503\n",
      "Model loaded from checkpoints/tokenformer_base.pth\n",
      "Model saved to checkpoints/tokenformer_scaled_step_1.pth\n",
      "Step 1: Model parameters after scaling: 699103\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_1.pth\n",
      "Step 1/3, Fine-tune Epoch 1/5, Train Loss: 3.2255, Eval Loss: 2.9880, Perplexity: 19.8462\n",
      "Step 1/3, Fine-tune Epoch 2/5, Train Loss: 3.1109, Eval Loss: 2.8950, Perplexity: 18.0842\n",
      "Step 1/3, Fine-tune Epoch 3/5, Train Loss: 3.0321, Eval Loss: 2.8194, Perplexity: 16.7665\n",
      "Step 1/3, Fine-tune Epoch 4/5, Train Loss: 2.9692, Eval Loss: 2.7574, Perplexity: 15.7586\n",
      "Step 1/3, Fine-tune Epoch 5/5, Train Loss: 2.9160, Eval Loss: 2.7083, Perplexity: 15.0035\n",
      "Model saved to checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2: Model parameters after scaling: 700703\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2/3, Fine-tune Epoch 1/5, Train Loss: 2.8868, Eval Loss: 2.6467, Perplexity: 14.1073\n",
      "Step 2/3, Fine-tune Epoch 2/5, Train Loss: 2.8408, Eval Loss: 2.6104, Perplexity: 13.6050\n",
      "Step 2/3, Fine-tune Epoch 3/5, Train Loss: 2.8079, Eval Loss: 2.5764, Perplexity: 13.1499\n",
      "Step 2/3, Fine-tune Epoch 4/5, Train Loss: 2.7778, Eval Loss: 2.5487, Perplexity: 12.7902\n",
      "Step 2/3, Fine-tune Epoch 5/5, Train Loss: 2.7514, Eval Loss: 2.5207, Perplexity: 12.4378\n",
      "Model saved to checkpoints/tokenformer_scaled_step_3.pth\n",
      "Step 3: Model parameters after scaling: 702303\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_3.pth\n",
      "Step 3/3, Fine-tune Epoch 1/5, Train Loss: 2.7484, Eval Loss: 2.4865, Perplexity: 12.0193\n",
      "Step 3/3, Fine-tune Epoch 2/5, Train Loss: 2.7201, Eval Loss: 2.4691, Perplexity: 11.8118\n",
      "Step 3/3, Fine-tune Epoch 3/5, Train Loss: 2.7002, Eval Loss: 2.4489, Perplexity: 11.5754\n",
      "Step 3/3, Fine-tune Epoch 4/5, Train Loss: 2.6822, Eval Loss: 2.4327, Perplexity: 11.3893\n",
      "Step 3/3, Fine-tune Epoch 5/5, Train Loss: 2.6650, Eval Loss: 2.4140, Perplexity: 11.1786\n",
      "Progressive scaling and fine-tuning time: 2213.49 seconds\n",
      "Total training time: 2806.10 seconds\n",
      "Model saved to checkpoints/tokenformer_final_scaled.pth\n",
      "Final scaled model saved to checkpoints/tokenformer_final_scaled.pth\n"
     ]
    }
   ],
   "source": [
    "progressive_train_and_scale(\n",
    "    file_path='training/pokemon.txt',\n",
    "    vocab_size=10000,\n",
    "    max_seq_len=50,\n",
    "    hidden_dim=32,\n",
    "    num_heads=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    scaling_steps=3,\n",
    "    new_tokens_per_step=5,\n",
    "    use_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
