{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.tokenformer import TokenformerLayer\n",
    "from models.transformer import TransformerModel\n",
    "from training.train import train \n",
    "from training.train import train_with_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste des TokenFormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Modèle : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 512, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 13_560_224\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 2048, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 21_424_544\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 4096, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 31_910_304\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 8192, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 52_881_824\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 16384, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 94_824_864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_num_init = 512\n",
    "\n",
    "model_configs = [\n",
    "    {\n",
    "        \"hidden_dim\": 512,  \n",
    "        \"num_heads\": 1,     \n",
    "        \"num_layers\": 12,     \n",
    "        \"max_seq_len\": 32,   \n",
    "        \"token_num\": token_num_init,     \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*4,  # 4x plus de tokens paramétriques\n",
    "        \"vocab_size\": 10656\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*8,  # 8x plus de tokens paramétriques\n",
    "        \"vocab_size\": 10656\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*16,  # 16x plus de tokens paramétriques\n",
    "        \"vocab_size\": 10656\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*32,  # 16x plus de tokens paramétriques\n",
    "        \"vocab_size\": 10656\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tester chaque configuration\n",
    "for config in model_configs:\n",
    "    model = TokenformerLayer(\n",
    "        hidden_size=config[\"hidden_dim\"],\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        num_attention_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"],\n",
    "        token_num=config[\"token_num\"]\n",
    "    )\n",
    "\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"📊 Modèle : {config}\")\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"{name}: {param.numel():_} paramètres\")\n",
    "\n",
    "    print(f\"🔹 PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste des Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Modèle : {'hidden_dim': 256, 'num_heads': 4, 'num_layers': 10, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 13_372_320\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 372, 'num_heads': 6, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 21_274_176\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 480, 'num_heads': 8, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 32_424_096\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 500, 'num_heads': 10, 'num_layers': 14, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 52_773_656\n",
      "\n",
      "📊 Modèle : {'hidden_dim': 612, 'num_heads': 12, 'num_layers': 18, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 94_117_896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_model_configs = [\n",
    "    {\n",
    "        \"hidden_dim\": 256,  \n",
    "        \"num_heads\": 4,   \n",
    "        \"num_layers\": 10,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656 \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 372, \n",
    "        \"num_heads\": 6,   \n",
    "        \"num_layers\": 8, \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 480,  \n",
    "        \"num_heads\": 8,   \n",
    "        \"num_layers\": 8,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 500,  \n",
    "        \"num_heads\": 10,   \n",
    "        \"num_layers\": 14,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 612,  \n",
    "        \"num_heads\": 12,   \n",
    "        \"num_layers\": 18,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    }\n",
    "]\n",
    "\n",
    "# Tester chaque configuration\n",
    "for config in model_configs:\n",
    "    model = TransformerModel(\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"]\n",
    "    )\n",
    "\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"📊 Modèle : {config}\")\n",
    "    print(f\"🔹 PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Modèle : {'hidden_dim': 256, 'num_heads': 4, 'num_layers': 10, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 13_372_320\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 13_372_320\n",
      "Approximate Computational Cost (FLOPS): 377_487_360\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6059 | Val Loss: 0.2070 | Val Perplexity: 1.2300 | Time: 40.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.1866 | Val Loss: 0.2019 | Val Perplexity: 1.2237 | Time: 40.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1626 | Val Loss: 0.2015 | Val Perplexity: 1.2232 | Time: 40.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1360 | Val Loss: 0.2077 | Val Perplexity: 1.2309 | Time: 40.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.1076 | Val Loss: 0.2235 | Val Perplexity: 1.2504 | Time: 40.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0811 | Val Loss: 0.2409 | Val Perplexity: 1.2724 | Time: 40.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0603 | Val Loss: 0.2572 | Val Perplexity: 1.2933 | Time: 40.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0460 | Val Loss: 0.2767 | Val Perplexity: 1.3187 | Time: 40.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0358 | Val Loss: 0.2991 | Val Perplexity: 1.3486 | Time: 40.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0304 | Val Loss: 0.3126 | Val Perplexity: 1.3670 | Time: 40.62s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.3111 | Test Perplexity: 1.3649\n",
      "Total Training Time: 405.94s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_13372320.pth\n",
      "📊 Modèle : {'hidden_dim': 372, 'num_heads': 6, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 21_274_176\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 21_274_176\n",
      "Approximate Computational Cost (FLOPS): 923_369_472\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5558 | Val Loss: 0.2135 | Val Perplexity: 1.2380 | Time: 43.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.1871 | Val Loss: 0.2011 | Val Perplexity: 1.2228 | Time: 43.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1561 | Val Loss: 0.2064 | Val Perplexity: 1.2293 | Time: 43.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1255 | Val Loss: 0.2163 | Val Perplexity: 1.2414 | Time: 43.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.0933 | Val Loss: 0.2358 | Val Perplexity: 1.2660 | Time: 43.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0660 | Val Loss: 0.2593 | Val Perplexity: 1.2960 | Time: 43.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0471 | Val Loss: 0.2826 | Val Perplexity: 1.3265 | Time: 43.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0352 | Val Loss: 0.3075 | Val Perplexity: 1.3601 | Time: 43.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0287 | Val Loss: 0.3309 | Val Perplexity: 1.3922 | Time: 43.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0243 | Val Loss: 0.3476 | Val Perplexity: 1.4157 | Time: 43.79s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.3443 | Test Perplexity: 1.4110\n",
      "Total Training Time: 438.19s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_21274176.pth\n",
      "📊 Modèle : {'hidden_dim': 480, 'num_heads': 8, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 32_424_096\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 32_424_096\n",
      "Approximate Computational Cost (FLOPS): 2_013_265_920\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5585 | Val Loss: 0.2259 | Val Perplexity: 1.2535 | Time: 56.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.2147 | Val Loss: 0.2277 | Val Perplexity: 1.2557 | Time: 56.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1626 | Val Loss: 0.2262 | Val Perplexity: 1.2538 | Time: 56.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1073 | Val Loss: 0.2533 | Val Perplexity: 1.2883 | Time: 56.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.0823 | Val Loss: 0.2853 | Val Perplexity: 1.3302 | Time: 56.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0656 | Val Loss: 0.3092 | Val Perplexity: 1.3624 | Time: 56.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0541 | Val Loss: 0.3350 | Val Perplexity: 1.3979 | Time: 56.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0430 | Val Loss: 0.3660 | Val Perplexity: 1.4419 | Time: 56.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0360 | Val Loss: 0.4054 | Val Perplexity: 1.5000 | Time: 56.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0318 | Val Loss: 0.4291 | Val Perplexity: 1.5358 | Time: 56.15s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.4311 | Test Perplexity: 1.5390\n",
      "Total Training Time: 562.43s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_32424096.pth\n",
      "📊 Modèle : {'hidden_dim': 500, 'num_heads': 10, 'num_layers': 14, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 52_773_656\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 52_773_656\n",
      "Approximate Computational Cost (FLOPS): 4_766_720_000\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6610 | Val Loss: 0.2242 | Val Perplexity: 1.2513 | Time: 93.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.2720 | Val Loss: 0.3766 | Val Perplexity: 1.4573 | Time: 93.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.3334 | Val Loss: 0.2453 | Val Perplexity: 1.2780 | Time: 93.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1765 | Val Loss: 0.2356 | Val Perplexity: 1.2656 | Time: 93.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.1305 | Val Loss: 0.2670 | Val Perplexity: 1.3060 | Time: 93.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.1132 | Val Loss: 0.2873 | Val Perplexity: 1.3328 | Time: 93.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0993 | Val Loss: 0.3115 | Val Perplexity: 1.3655 | Time: 93.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0790 | Val Loss: 0.3377 | Val Perplexity: 1.4018 | Time: 93.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0665 | Val Loss: 0.3683 | Val Perplexity: 1.4453 | Time: 93.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0573 | Val Loss: 0.4031 | Val Perplexity: 1.4964 | Time: 93.23s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.4040 | Test Perplexity: 1.4979\n",
      "Total Training Time: 933.89s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_52773656.pth\n",
      "📊 Modèle : {'hidden_dim': 612, 'num_heads': 12, 'num_layers': 18, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "🔹 PyTorch Trainable Params : 94_117_896\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 94_117_896\n",
      "Approximate Computational Cost (FLOPS): 10_896_850_944\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.8115 | Val Loss: 0.2350 | Val Perplexity: 1.2649 | Time: 155.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 1.3584 | Val Loss: 3.3948 | Val Perplexity: 29.8101 | Time: 155.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 1.2077 | Val Loss: 0.4330 | Val Perplexity: 1.5419 | Time: 154.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.3379 | Val Loss: 0.3626 | Val Perplexity: 1.4371 | Time: 154.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.2537 | Val Loss: 0.3788 | Val Perplexity: 1.4605 | Time: 154.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.2256 | Val Loss: 0.4002 | Val Perplexity: 1.4921 | Time: 154.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.2026 | Val Loss: 0.4202 | Val Perplexity: 1.5222 | Time: 154.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.1760 | Val Loss: 0.4401 | Val Perplexity: 1.5528 | Time: 154.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.1568 | Val Loss: 0.4726 | Val Perplexity: 1.6041 | Time: 154.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.1342 | Val Loss: 0.5077 | Val Perplexity: 1.6614 | Time: 154.47s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.5096 | Test Perplexity: 1.6646\n",
      "Total Training Time: 1547.84s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_94117896.pth\n"
     ]
    }
   ],
   "source": [
    "for config in transformer_model_configs:\n",
    "    # Création dynamique des noms de fichiers\n",
    "    model = TransformerModel(\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"]\n",
    "    )\n",
    "    \n",
    "    # Calcul des paramètres entraînables\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    checkpoint_filename = f\"checkpoint_{trainable_params_auto}.pth\"\n",
    "    results_filename = f\"results_{trainable_params_auto}.csv\"\n",
    "\n",
    "    print(f\"📊 Modèle : {config}\")\n",
    "    print(f\"🔹 PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n",
    "    \n",
    "    # Lancer l'entraînement\n",
    "    train(\n",
    "        use_tokenformer=False,\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        max_seq_len=config[\"max_seq_len\"],\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001,\n",
    "        token_num=32,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.2,\n",
    "        model_checkpoint_file_name=checkpoint_filename,\n",
    "        model_results_file_name=results_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TokenFormers : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling progressif\n",
    "On entraîne TokenFormer avec token_num = 512, puis on scale progressivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Initial Token Num: 512\n",
      "Trainable Parameters: 13_560_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Epoch 1/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 27.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Epoch 2/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 26.92s\n",
      "\n",
      "🔍 Evaluation finale après Scaling Step 0...\n",
      "Final Evaluation for Step 0 | Test Loss: nan | Test Perplexity: nan\n",
      "✅ Modèle sauvegardé : Saved_Models_Checkpoints\\tokenformer_scaled_step_0.pth\n",
      "✅ TokenFormer scalé à 2048 tokens.\n",
      "🔼 Scaling step 1: token_num = 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Epoch 1/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 40.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Epoch 2/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 40.60s\n",
      "\n",
      "🔍 Evaluation finale après Scaling Step 1...\n",
      "Final Evaluation for Step 1 | Test Loss: nan | Test Perplexity: nan\n",
      "✅ Modèle sauvegardé : Saved_Models_Checkpoints\\tokenformer_scaled_step_1.pth\n",
      "✅ TokenFormer scalé à 4096 tokens.\n",
      "🔼 Scaling step 2: token_num = 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Epoch 1/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 60.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Epoch 2/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 60.71s\n",
      "\n",
      "🔍 Evaluation finale après Scaling Step 2...\n",
      "Final Evaluation for Step 2 | Test Loss: nan | Test Perplexity: nan\n",
      "✅ Modèle sauvegardé : Saved_Models_Checkpoints\\tokenformer_scaled_step_2.pth\n",
      "✅ TokenFormer scalé à 8192 tokens.\n",
      "🔼 Scaling step 3: token_num = 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Epoch 1/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 99.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Epoch 2/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 99.78s\n",
      "\n",
      "🔍 Evaluation finale après Scaling Step 3...\n",
      "Final Evaluation for Step 3 | Test Loss: nan | Test Perplexity: nan\n",
      "✅ Modèle sauvegardé : Saved_Models_Checkpoints\\tokenformer_scaled_step_3.pth\n",
      "✅ TokenFormer scalé à 16384 tokens.\n",
      "🔼 Scaling step 4: token_num = 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Epoch 1/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 181.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Epoch 2/2 | Train Loss: nan | Val Loss: nan | Val Perplexity: nan | Time: 178.98s\n",
      "\n",
      "🔍 Evaluation finale après Scaling Step 4...\n",
      "Final Evaluation for Step 4 | Test Loss: nan | Test Perplexity: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m token_num_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_with_scaling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/pokemon.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_token_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_num_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# On va jusqu'à 16384 tokens\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_tokens_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtoken_num_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_num_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_num_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_num_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\wsl.localhost\\Ubuntu\\home\\hocine\\GitHub\\TokenFormer-Reimplementation\\tokenformer\\training\\train.py:290\u001b[0m, in \u001b[0;36mtrain_with_scaling\u001b[1;34m(file_path, initial_token_num, scaling_steps, new_tokens_per_step, hidden_dim, num_heads, max_seq_len, batch_size, num_epochs, learning_rate, val_ratio, test_ratio, model_base_name)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Sauvegarde du modèle après chaque scaling step\u001b[39;00m\n\u001b[0;32m    289\u001b[0m model_checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_base_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_step_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 290\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Modèle sauvegardé : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_checkpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Ajout des résultats de test dans le fichier CSV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\torch\\serialization.py:944\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 944\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\torch\\serialization.py:1216\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token_num_init = 512\n",
    "\n",
    "train_with_scaling(\n",
    "    file_path='data/pokemon.txt',\n",
    "    initial_token_num=token_num_init,\n",
    "    scaling_steps=4,  # On va jusqu'à 16384 tokens\n",
    "    new_tokens_per_step=[token_num_init * 3, token_num_init * 4, token_num_init * 8, token_num_init * 16],  \n",
    "    hidden_dim=512,\n",
    "    num_heads=1,\n",
    "    max_seq_len=32,\n",
    "    batch_size=32,\n",
    "    num_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
