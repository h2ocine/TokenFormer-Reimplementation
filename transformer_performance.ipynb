{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 719039\n",
      "Vocabulary size: 10655\n",
      "Total training tokens: 27.65M\n",
      "开始训练\n",
      "Epoch 1/10, Train Loss: 3.1741, Eval Loss: 0.5180, Perplexity: 1.6787, Train Time: 106.33s, Eval Time: 28.22s\n",
      "Epoch 2/10, Train Loss: 0.2924, Eval Loss: 0.2090, Perplexity: 1.2325, Train Time: 89.34s, Eval Time: 46.21s\n",
      "Epoch 3/10, Train Loss: 0.2021, Eval Loss: 0.1890, Perplexity: 1.2081, Train Time: 110.98s, Eval Time: 43.50s\n",
      "Epoch 4/10, Train Loss: 0.1905, Eval Loss: 0.1807, Perplexity: 1.1981, Train Time: 110.59s, Eval Time: 43.74s\n",
      "Epoch 5/10, Train Loss: 0.1840, Eval Loss: 0.1742, Perplexity: 1.1903, Train Time: 112.92s, Eval Time: 42.81s\n",
      "Epoch 6/10, Train Loss: 0.1787, Eval Loss: 0.1701, Perplexity: 1.1854, Train Time: 110.73s, Eval Time: 41.87s\n",
      "Epoch 7/10, Train Loss: 0.1744, Eval Loss: 0.1650, Perplexity: 1.1794, Train Time: 108.98s, Eval Time: 43.56s\n",
      "Epoch 8/10, Train Loss: 0.1706, Eval Loss: 0.1616, Perplexity: 1.1754, Train Time: 114.55s, Eval Time: 43.05s\n",
      "Epoch 9/10, Train Loss: 0.1672, Eval Loss: 0.1577, Perplexity: 1.1708, Train Time: 111.89s, Eval Time: 56.69s\n",
      "Epoch 10/10, Train Loss: 0.1641, Eval Loss: 0.1542, Perplexity: 1.1667, Train Time: 147.56s, Eval Time: 55.74s\n",
      "total training time (without evaluation): 1123.87秒\n",
      "total time (with eval): 1569.28秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import time\n",
    "\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, max_seq_len):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        vocab = sorted(set(self.text))\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.itos = {idx: word for word, idx in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        x = seq[:-1]\n",
    "        y = seq[1:]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd=128, n_layer=2, n_head=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = token_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # compute the loss\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "        q = self.query(x).view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "        v = self.value(x).view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        return y\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# training loop\n",
    "# -------------------------------\n",
    "\n",
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            logits, _ = model(x_batch)\n",
    "            # logits shape: (batch_size, seq_len, vocab_size)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            y_batch = y_batch.view(-1)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)\n",
    "            total_tokens += y_batch.size(0)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    import time\n",
    "    import torch\n",
    "    from torch import nn, optim\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # use M1 GPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # use CPU if no M1 GPU\n",
    "   \n",
    "    max_seq_len = 32  # the maximum sequence length\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    n_embd = 32\n",
    "    n_layer = 2\n",
    "    n_head = 4\n",
    "\n",
    "    # \n",
    "    dataset = PokemonDataset(file_path='pokemon.txt', max_seq_len=max_seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # initialize the model\n",
    "    model = GPTLanguageModel(vocab_size=dataset.vocab_size,\n",
    "                             block_size=max_seq_len - 1,\n",
    "                             n_embd=n_embd,\n",
    "                             n_layer=n_layer,\n",
    "                             n_head=n_head)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(\"Model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"Vocabulary size:\", dataset.vocab_size)\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "\n",
    "    # define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"Debut d'entrainement\")\n",
    "    total_training_time = 0.0  # record the total training time\n",
    "    overall_start_time = time.time()  # record the start time of the whole training process\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # record the start time of this epoch\n",
    "        epoch_train_start = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            # forward pass\n",
    "            _, loss = model(x_batch, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # compute the time taken for this epoch\n",
    "        epoch_train_time = time.time() - epoch_train_start\n",
    "        total_training_time += epoch_train_time\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        \n",
    "        if use_metrics:\n",
    "            # compute perplexity on the validation set\n",
    "            eval_start = time.time()\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            eval_time = time.time() - eval_start\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Cumul average Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Train average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, \"\n",
    "                  f\"Train Time: {epoch_train_time:.2f}s, Eval Time: {eval_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train average Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Train Time: {epoch_train_time:.2f}s\")\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    print(f\"total training time (without evaluation): {total_training_time:.2f}s\")\n",
    "    print(f\"total time (with eval): {overall_end_time - overall_start_time:.2f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(use_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
