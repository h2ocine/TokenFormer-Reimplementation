{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 409917\n",
      "Sample input: tensor([ 60731, 234155, 346236, 384786,  71337, 200785, 265126,  64556, 192909,\n",
      "         71713, 354098, 358183,  15467, 369931,  94274, 209109, 281178, 202105,\n",
      "         88883, 259525, 223774,  84455, 403675, 215483, 202105, 176896, 100388,\n",
      "        354098, 180708, 265126,  86176, 193506, 354098, 122588,  71713, 183548,\n",
      "        209912, 163456,  29267, 279409, 354098, 112154, 353913, 379347, 354098,\n",
      "        106983, 265126,  64556, 192922])\n",
      "Sample target: tensor([234155, 346236, 384786,  71337, 200785, 265126,  64556, 192909,  71713,\n",
      "        354098, 358183,  15467, 369931,  94274, 209109, 281178, 202105,  88883,\n",
      "        259525, 223774,  84455, 403675, 215483, 202105, 176896, 100388, 354098,\n",
      "        180708, 265126,  86176, 193506, 354098, 122588,  71713, 183548, 209912,\n",
      "        163456,  29267, 279409, 354098, 112154, 353913, 379347, 354098, 106983,\n",
      "        265126,  64556, 192922, 151537])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class OpenWebTextDataset(Dataset):\n",
    "    def __init__(self, max_seq_len):\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "\n",
    "        # Combine all text into a single string\n",
    "        self.text = \" \".join(dataset[\"text\"]).lower().split()\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocab = sorted(set(self.text))\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# Example usage\n",
    "max_seq_len = 50\n",
    "dataset = OpenWebTextDataset(max_seq_len)\n",
    "print(\"Vocabulary size:\", dataset.vocab_size)\n",
    "print(\"Sample input:\", dataset[0][0])\n",
    "print(\"Sample target:\", dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1,token_num=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        token_num=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "            token_num=token_num\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "        # to obtain logits (before softmax) for the vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        # Linear layer for logits\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass through the model.\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "            y_batch = y_batch.view(-1)  # Flatten targets to shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss for the current batch.\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)  # Multiply by the number of tokens.\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    # Calculate the average loss per token.\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # Compute the perplexity by exponentiating the average loss.\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    # Configurations\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # use M1 GPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # use CPU if no M1 GPU\n",
    "    print('Device:', device)\n",
    "\n",
    "    hidden_dim = 32\n",
    "    num_heads = 4\n",
    "    max_seq_len = 16  # Réduction de la taille des séquences\n",
    "    batch_size = 8  # Batch réduit\n",
    "    num_epochs = 5  # Moins d'époques pour tester plus vite\n",
    "    learning_rate = 0.001\n",
    "    token_num = 32\n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = OpenWebTextDataset(max_seq_len=max_seq_len)\n",
    "\n",
    "    # Réduction du dataset\n",
    "    subset_size = 100_000  # Choisir un sous-ensemble plus petit\n",
    "    dataset = Subset(dataset, range(min(subset_size, len(dataset))))\n",
    "\n",
    "    vocab_size = dataset.dataset.vocab_size  # Mise à jour du vocab_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(\"Dataset réduit à :\", len(dataset))\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len, token_num=token_num)\n",
    "    model = model.to(device)\n",
    "    print(\"Model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"Début de l'entraînement\")\n",
    "    total_training_time = 0.0\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_train_start = time.time()\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for x_batch, y_batch in progress_bar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_training_time = time.time() - epoch_train_start\n",
    "        total_training_time += epoch_training_time\n",
    "\n",
    "        if use_metrics:\n",
    "            eval_start = time.time()\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            eval_time = time.time() - eval_start\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s, Eval Time: {eval_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s\")\n",
    "\n",
    "    print(f\"Total training time (excluding evaluation): {total_training_time:.2f} seconds\")\n",
    "    # Sauvegarde du modèle après entraînement\n",
    "    model_save_path = \"tokenformer_model.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Modèle sauvegardé dans : {model_save_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "num_heads = 4\n",
    "max_seq_len = 16  # Réduction de la taille des séquences\n",
    "batch_size = 8  # Batch réduit\n",
    "num_epochs = 3  # Moins d'époques pour tester plus vite\n",
    "learning_rate = 0.001\n",
    "token_num = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'échantillons : 8193712\n"
     ]
    }
   ],
   "source": [
    "# Préparer les données\n",
    "dataset = OpenWebTextDataset(max_seq_len=max_seq_len)\n",
    "\n",
    "print(\"Nombre total d'échantillons :\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dataset réduit à : 100000\n",
      "Vocab size: 409917\n",
      "Model parameters: 26655485\n",
      "Début de l'entraînement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 3.2687, Eval Loss: 0.8543, Perplexity: 2.3497, Train Time: 244.47s, Eval Time: 66.78s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 0.8479, Eval Loss: 0.5063, Perplexity: 1.6592, Train Time: 235.62s, Eval Time: 63.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 0.6201, Eval Loss: 0.4513, Perplexity: 1.5703, Train Time: 253.95s, Eval Time: 60.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 0.5538, Eval Loss: 0.4309, Perplexity: 1.5387, Train Time: 247.08s, Eval Time: 63.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.5202, Eval Loss: 0.4167, Perplexity: 1.5169, Train Time: 238.87s, Eval Time: 68.79s\n",
      "Total training time (excluding evaluation): 1220.00 seconds\n"
     ]
    }
   ],
   "source": [
    "train(use_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
