{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.tokenformer import TokenformerLayer\n",
    "from models.transformer import TransformerModel\n",
    "from training.train import train \n",
    "from training.train import train_with_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste des TokenFormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 512, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 13_560_224\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 2048, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 21_424_544\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 4096, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 31_910_304\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 8192, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 52_881_824\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 512, 'num_heads': 1, 'num_layers': 12, 'max_seq_len': 32, 'token_num': 16384, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 94_824_864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_num_init = 512\n",
    "\n",
    "model_configs = [\n",
    "    {\n",
    "        \"hidden_dim\": 512,  \n",
    "        \"num_heads\": 1,     \n",
    "        \"num_layers\": 12,     \n",
    "        \"max_seq_len\": 32,   \n",
    "        \"token_num\": token_num_init,     \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*4,  # 4x plus de tokens paramÃ©triques\n",
    "        \"vocab_size\": 10656\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*8,  # 8x plus de tokens paramÃ©triques\n",
    "        \"vocab_size\": 10656\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*16,  # 16x plus de tokens paramÃ©triques\n",
    "        \"vocab_size\": 10656\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"token_num\": token_num_init*32,  # 16x plus de tokens paramÃ©triques\n",
    "        \"vocab_size\": 10656\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tester chaque configuration\n",
    "for config in model_configs:\n",
    "    model = TokenformerLayer(\n",
    "        hidden_size=config[\"hidden_dim\"],\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        num_attention_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"],\n",
    "        token_num=config[\"token_num\"]\n",
    "    )\n",
    "\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"ðŸ“Š ModÃ¨le : {config}\")\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"{name}: {param.numel():_} paramÃ¨tres\")\n",
    "\n",
    "    print(f\"ðŸ”¹ PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste des Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 256, 'num_heads': 4, 'num_layers': 10, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 13_372_320\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 372, 'num_heads': 6, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 21_274_176\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 480, 'num_heads': 8, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 32_424_096\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 500, 'num_heads': 10, 'num_layers': 14, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 52_773_656\n",
      "\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 612, 'num_heads': 12, 'num_layers': 18, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 94_117_896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_model_configs = [\n",
    "    {\n",
    "        \"hidden_dim\": 256,  \n",
    "        \"num_heads\": 4,   \n",
    "        \"num_layers\": 10,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656 \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 372, \n",
    "        \"num_heads\": 6,   \n",
    "        \"num_layers\": 8, \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 480,  \n",
    "        \"num_heads\": 8,   \n",
    "        \"num_layers\": 8,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 500,  \n",
    "        \"num_heads\": 10,   \n",
    "        \"num_layers\": 14,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 612,  \n",
    "        \"num_heads\": 12,   \n",
    "        \"num_layers\": 18,  \n",
    "        \"max_seq_len\": 32,  \n",
    "        \"vocab_size\": 10656  \n",
    "    }\n",
    "]\n",
    "\n",
    "# Tester chaque configuration\n",
    "for config in model_configs:\n",
    "    model = TransformerModel(\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"]\n",
    "    )\n",
    "\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"ðŸ“Š ModÃ¨le : {config}\")\n",
    "    print(f\"ðŸ”¹ PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 256, 'num_heads': 4, 'num_layers': 10, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 13_372_320\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 13_372_320\n",
      "Approximate Computational Cost (FLOPS): 377_487_360\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6059 | Val Loss: 0.2070 | Val Perplexity: 1.2300 | Time: 40.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.1866 | Val Loss: 0.2019 | Val Perplexity: 1.2237 | Time: 40.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1626 | Val Loss: 0.2015 | Val Perplexity: 1.2232 | Time: 40.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1360 | Val Loss: 0.2077 | Val Perplexity: 1.2309 | Time: 40.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.1076 | Val Loss: 0.2235 | Val Perplexity: 1.2504 | Time: 40.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0811 | Val Loss: 0.2409 | Val Perplexity: 1.2724 | Time: 40.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0603 | Val Loss: 0.2572 | Val Perplexity: 1.2933 | Time: 40.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0460 | Val Loss: 0.2767 | Val Perplexity: 1.3187 | Time: 40.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0358 | Val Loss: 0.2991 | Val Perplexity: 1.3486 | Time: 40.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0304 | Val Loss: 0.3126 | Val Perplexity: 1.3670 | Time: 40.62s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.3111 | Test Perplexity: 1.3649\n",
      "Total Training Time: 405.94s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_13372320.pth\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 372, 'num_heads': 6, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 21_274_176\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 21_274_176\n",
      "Approximate Computational Cost (FLOPS): 923_369_472\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5558 | Val Loss: 0.2135 | Val Perplexity: 1.2380 | Time: 43.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.1871 | Val Loss: 0.2011 | Val Perplexity: 1.2228 | Time: 43.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1561 | Val Loss: 0.2064 | Val Perplexity: 1.2293 | Time: 43.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1255 | Val Loss: 0.2163 | Val Perplexity: 1.2414 | Time: 43.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.0933 | Val Loss: 0.2358 | Val Perplexity: 1.2660 | Time: 43.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0660 | Val Loss: 0.2593 | Val Perplexity: 1.2960 | Time: 43.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0471 | Val Loss: 0.2826 | Val Perplexity: 1.3265 | Time: 43.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0352 | Val Loss: 0.3075 | Val Perplexity: 1.3601 | Time: 43.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0287 | Val Loss: 0.3309 | Val Perplexity: 1.3922 | Time: 43.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0243 | Val Loss: 0.3476 | Val Perplexity: 1.4157 | Time: 43.79s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.3443 | Test Perplexity: 1.4110\n",
      "Total Training Time: 438.19s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_21274176.pth\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 480, 'num_heads': 8, 'num_layers': 8, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 32_424_096\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 32_424_096\n",
      "Approximate Computational Cost (FLOPS): 2_013_265_920\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5585 | Val Loss: 0.2259 | Val Perplexity: 1.2535 | Time: 56.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.2147 | Val Loss: 0.2277 | Val Perplexity: 1.2557 | Time: 56.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1626 | Val Loss: 0.2262 | Val Perplexity: 1.2538 | Time: 56.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1073 | Val Loss: 0.2533 | Val Perplexity: 1.2883 | Time: 56.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.0823 | Val Loss: 0.2853 | Val Perplexity: 1.3302 | Time: 56.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0656 | Val Loss: 0.3092 | Val Perplexity: 1.3624 | Time: 56.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0541 | Val Loss: 0.3350 | Val Perplexity: 1.3979 | Time: 56.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0430 | Val Loss: 0.3660 | Val Perplexity: 1.4419 | Time: 56.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0360 | Val Loss: 0.4054 | Val Perplexity: 1.5000 | Time: 56.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0318 | Val Loss: 0.4291 | Val Perplexity: 1.5358 | Time: 56.15s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.4311 | Test Perplexity: 1.5390\n",
      "Total Training Time: 562.43s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_32424096.pth\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 500, 'num_heads': 10, 'num_layers': 14, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 52_773_656\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 52_773_656\n",
      "Approximate Computational Cost (FLOPS): 4_766_720_000\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6610 | Val Loss: 0.2242 | Val Perplexity: 1.2513 | Time: 93.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.2720 | Val Loss: 0.3766 | Val Perplexity: 1.4573 | Time: 93.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.3334 | Val Loss: 0.2453 | Val Perplexity: 1.2780 | Time: 93.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.1765 | Val Loss: 0.2356 | Val Perplexity: 1.2656 | Time: 93.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.1305 | Val Loss: 0.2670 | Val Perplexity: 1.3060 | Time: 93.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.1132 | Val Loss: 0.2873 | Val Perplexity: 1.3328 | Time: 93.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0993 | Val Loss: 0.3115 | Val Perplexity: 1.3655 | Time: 93.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.0790 | Val Loss: 0.3377 | Val Perplexity: 1.4018 | Time: 93.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.0665 | Val Loss: 0.3683 | Val Perplexity: 1.4453 | Time: 93.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.0573 | Val Loss: 0.4031 | Val Perplexity: 1.4964 | Time: 93.23s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.4040 | Test Perplexity: 1.4979\n",
      "Total Training Time: 933.89s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_52773656.pth\n",
      "ðŸ“Š ModÃ¨le : {'hidden_dim': 612, 'num_heads': 12, 'num_layers': 18, 'max_seq_len': 32, 'vocab_size': 10656}\n",
      "ðŸ”¹ PyTorch Trainable Params : 94_117_896\n",
      "\n",
      "Training on cuda\n",
      "Vocab Size: 10656\n",
      "Model Type: Transformer\n",
      "Trainable Parameters: 94_117_896\n",
      "Approximate Computational Cost (FLOPS): 10_896_850_944\n",
      "Training Samples: 53524, Validation Samples: 17841, Test Samples: 17841\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.8115 | Val Loss: 0.2350 | Val Perplexity: 1.2649 | Time: 155.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 1.3584 | Val Loss: 3.3948 | Val Perplexity: 29.8101 | Time: 155.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 1.2077 | Val Loss: 0.4330 | Val Perplexity: 1.5419 | Time: 154.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.3379 | Val Loss: 0.3626 | Val Perplexity: 1.4371 | Time: 154.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.2537 | Val Loss: 0.3788 | Val Perplexity: 1.4605 | Time: 154.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.2256 | Val Loss: 0.4002 | Val Perplexity: 1.4921 | Time: 154.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.2026 | Val Loss: 0.4202 | Val Perplexity: 1.5222 | Time: 154.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.1760 | Val Loss: 0.4401 | Val Perplexity: 1.5528 | Time: 154.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.1568 | Val Loss: 0.4726 | Val Perplexity: 1.6041 | Time: 154.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.1342 | Val Loss: 0.5077 | Val Perplexity: 1.6614 | Time: 154.47s\n",
      "\n",
      "Final evaluation on test set...\n",
      "Test Loss: 0.5096 | Test Perplexity: 1.6646\n",
      "Total Training Time: 1547.84s\n",
      "Final model saved: Saved_Models_Checkpoints\\checkpoint_94117896.pth\n"
     ]
    }
   ],
   "source": [
    "for config in transformer_model_configs:\n",
    "    # CrÃ©ation dynamique des noms de fichiers\n",
    "    model = TransformerModel(\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        max_seq_len=config[\"max_seq_len\"]\n",
    "    )\n",
    "    \n",
    "    # Calcul des paramÃ¨tres entraÃ®nables\n",
    "    trainable_params_auto = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    checkpoint_filename = f\"checkpoint_{trainable_params_auto}.pth\"\n",
    "    results_filename = f\"results_{trainable_params_auto}.csv\"\n",
    "\n",
    "    print(f\"ðŸ“Š ModÃ¨le : {config}\")\n",
    "    print(f\"ðŸ”¹ PyTorch Trainable Params : {trainable_params_auto:_}\\n\")\n",
    "    \n",
    "    # Lancer l'entraÃ®nement\n",
    "    train(\n",
    "        use_tokenformer=False,\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        max_seq_len=config[\"max_seq_len\"],\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001,\n",
    "        token_num=32,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.2,\n",
    "        model_checkpoint_file_name=checkpoint_filename,\n",
    "        model_results_file_name=results_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TokenFormers : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling progressif\n",
    "On entraÃ®ne TokenFormer avec token_num = 512, puis on scale progressivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num_init = 512\n",
    "\n",
    "train_with_scaling(\n",
    "    file_path='data/pokemon.txt',\n",
    "    initial_token_num=token_num_init,\n",
    "    scaling_steps=4,  # On va jusqu'Ã  16384 tokens\n",
    "    new_tokens_per_step=[token_num_init * 3, token_num_init * 4, token_num_init * 8, token_num_init * 16],  \n",
    "    hidden_dim=512,\n",
    "    num_heads=1,\n",
    "    max_seq_len=32,\n",
    "    batch_size=32,\n",
    "    num_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
