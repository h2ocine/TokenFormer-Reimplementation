{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=10,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class remains unchanged\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab_size, max_seq_len):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.vocab = {word: idx for idx, word in enumerate(set(self.text))}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Configurations\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    vocab_size=10000    # Valeur temporaire, sera mise à jour avec vocab_size réel\n",
    "    hidden_dim=32\n",
    "    num_heads=4\n",
    "    #num_layers=2,\n",
    "    #num_tokens=10,\n",
    "    max_seq_len=50\n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = PokemonDataset(file_path='training/pokemon.txt', vocab_size=vocab_size, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size  # Mettre à jour vocab_size après avoir créé le vocabulaire\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"debut de l'entrainement\")\n",
    "    num_epochs=10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            device = next(model.parameters()).device\n",
    "\n",
    "            # Déplacer les données sur le même appareil que le modèle (GPU ou CPU)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            #print(\"outputs:\", outputs.shape)\n",
    "\n",
    "            # Aplatir les logits pour la CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, hidden_dim)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            #print(\"outputs aplati:\", outputs.shape)\n",
    "\n",
    "            # Aplatir les labels\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Afficher la perte moyenne par epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debut de l'entrainement\n",
      "Epoch 1/10, Loss: 0.0031865480706952044\n",
      "Epoch 2/10, Loss: 0.0009781118752288996\n",
      "Epoch 3/10, Loss: 0.0003403690678341826\n",
      "Epoch 4/10, Loss: 0.0001639128594239644\n",
      "Epoch 5/10, Loss: 8.936495099195651e-05\n",
      "Epoch 6/10, Loss: 6.470846814064477e-05\n",
      "Epoch 7/10, Loss: 4.1274382638995384e-05\n",
      "Epoch 8/10, Loss: 3.360526681994694e-05\n",
      "Epoch 9/10, Loss: 2.755846787782013e-05\n",
      "Epoch 10/10, Loss: 2.470227711210383e-05\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "\n",
    "    def add_new_params(self, num_new_tokens):\n",
    "        device = self.key_param_tokens.device  # Get the current device\n",
    "\n",
    "        new_key_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_key_dim), device=device))\n",
    "        new_value_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_value_dim), device=device))\n",
    "\n",
    "        # Concatenate with existing parameters\n",
    "        self.key_param_tokens = nn.Parameter(torch.cat([self.key_param_tokens, new_key_tokens], dim=0))\n",
    "        self.value_param_tokens = nn.Parameter(torch.cat([self.value_param_tokens, new_value_tokens], dim=0))\n",
    "\n",
    "        #print(f\"Added {num_new_tokens} new tokens. Total tokens now: {self.key_param_tokens.shape[0]}\")\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "def progressive_train_and_scale(\n",
    "    file_path,\n",
    "    vocab_size,\n",
    "    max_seq_len,\n",
    "    hidden_dim,\n",
    "    num_heads,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    scaling_steps,\n",
    "    new_tokens_per_step,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='checkpoints/tokenformer_base.pth',\n",
    "):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = PokemonDataset(file_path=file_path, vocab_size=vocab_size, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate the model and move to device\n",
    "    model = TokenformerLayer(hidden_size=hidden_dim, vocab_size=vocab_size, num_attention_heads=num_heads, max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initial Training and Saving\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch).view(-1, hidden_dim)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "    # Save the initial model\n",
    "    save_model(model, checkpoint_path)\n",
    "    print(f\"Initial model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    # Load model before scaling\n",
    "    load_model(model, checkpoint_path)\n",
    "\n",
    "    # Progressive scaling and fine-tuning\n",
    "    for step in range(scaling_steps):\n",
    "        # Add new parameters\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, Pattention):\n",
    "                module.add_new_params(new_tokens_per_step)\n",
    "\n",
    "        # Save the scaled model checkpoint\n",
    "        checkpoint_path_step = f'checkpoints/tokenformer_scaled_step_{step + 1}.pth'\n",
    "        save_model(model, checkpoint_path_step)\n",
    "\n",
    "        print(f\"Step {step + 1}: Model parameters after scaling: {count_parameters(model)}\")\n",
    "\n",
    "        # Load the latest checkpoint before fine-tuning\n",
    "        load_model(model, checkpoint_path_step)\n",
    "\n",
    "        # Fine-tuning for the current step\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).view(-1, hidden_dim)\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "    # Save the final scaled model checkpoint\n",
    "    final_checkpoint_path = 'checkpoints/tokenformer_final_scaled.pth'\n",
    "    save_model(model, final_checkpoint_path)\n",
    "    print(f\"Final scaled model saved to {final_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial training...\n",
      "Initial Training Epoch 1/5, Loss: 0.00405199784068704\n",
      "Initial Training Epoch 2/5, Loss: 0.0010964027286532532\n",
      "Initial Training Epoch 3/5, Loss: 0.00039806937101885217\n",
      "Initial Training Epoch 4/5, Loss: 0.00019192799218113225\n",
      "Initial Training Epoch 5/5, Loss: 0.00011736223036049437\n",
      "Model saved to checkpoints/tokenformer_base.pth\n",
      "Initial model parameters: 345888\n",
      "Model loaded from checkpoints/tokenformer_base.pth\n",
      "Model saved to checkpoints/tokenformer_scaled_step_1.pth\n",
      "Step 1: Model parameters after scaling: 347488\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/tk1z4ld57254_sgr1npxv2qm0000gn/T/ipykernel_25417/2821629227.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3, Fine-tune Epoch 1/5, Loss: 6.270982708548387e-05\n",
      "Step 1/3, Fine-tune Epoch 2/5, Loss: 4.9207377172659395e-05\n",
      "Step 1/3, Fine-tune Epoch 3/5, Loss: 4.154280187602042e-05\n",
      "Step 1/3, Fine-tune Epoch 4/5, Loss: 5.210743198133251e-05\n",
      "Step 1/3, Fine-tune Epoch 5/5, Loss: 4.2491013088074234e-05\n",
      "Model saved to checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2: Model parameters after scaling: 349088\n",
      "Model loaded from checkpoints/tokenformer_scaled_step_2.pth\n",
      "Step 2/3, Fine-tune Epoch 1/5, Loss: 3.3079449172570354e-05\n",
      "Step 2/3, Fine-tune Epoch 2/5, Loss: 2.6569022649267098e-05\n"
     ]
    }
   ],
   "source": [
    "progressive_train_and_scale(\n",
    "    file_path='training/pokemon.txt',\n",
    "    vocab_size=10000,\n",
    "    max_seq_len=50,\n",
    "    hidden_dim=32,\n",
    "    num_heads=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    scaling_steps=3,\n",
    "    new_tokens_per_step=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
