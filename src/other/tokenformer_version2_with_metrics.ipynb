{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_dropout=0.1,token_num=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        #print(hidden_size)\n",
    "        #print(num_attention_heads)\n",
    "\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Query, Key, and Value projections\n",
    "        self.query = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.key = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.value = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size // num_attention_heads,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "        self.out_proj = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            #n=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.norm_factor = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "\n",
    "        # Proceed with attention mechanism\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.query(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"qshape: \", query_layer.shape)\n",
    "        key_layer = self.key(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"kshape: \", key_layer.shape)\n",
    "        value_layer = self.value(hidden_states).view(\n",
    "            batch_size, seq_len, self.num_attention_heads, self.head_dim\n",
    "        )\n",
    "        #print(\"vshape: \", value_layer.shape)\n",
    "        query_layer = query_layer.transpose(1, 2)\n",
    "        key_layer = key_layer.transpose(1, 2)\n",
    "        value_layer = value_layer.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= self.norm_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.hidden_size\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(context_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenformerLayer(nn.Module):\n",
    "    \"\"\"A single Tokenformer layer implementing token-token and token-parameter interactions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        num_attention_heads,\n",
    "        max_seq_len,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        token_num=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden dimension.\n",
    "            num_attention_heads (int): Number of attention heads for multi-head attention.\n",
    "            num_param_tokens (int): Number of parameter tokens for the feed-forward Pattention layer.\n",
    "            attention_dropout (float): Dropout probability for attention weights.\n",
    "            hidden_dropout (float): Dropout probability for residual connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.input_layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)\n",
    "\n",
    "        # Self-attention using Pattention\n",
    "        self.attention = self.attention = SelfAttention(\n",
    "            #vocab_size=30522,  # Provide a valid vocab_size if needed\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            #max_seq_len=max_seq_len,\n",
    "            attention_dropout=attention_dropout,\n",
    "            token_num=token_num\n",
    "        )\n",
    "\n",
    "        # Feed-forward network using Pattention\n",
    "        self.mlp = Pattention(\n",
    "            d1=hidden_size,\n",
    "            d2=hidden_size,\n",
    "            n=token_num,\n",
    "            param_key_init_method=torch.nn.init.xavier_uniform_,\n",
    "            param_value_init_method=torch.nn.init.xavier_uniform_,\n",
    "            norm_activation_type=\"l2_norm_gelu\"\n",
    "        )\n",
    "\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "\n",
    "        # to obtain logits (before softmax) for the vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Tokenformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for self-attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        #print(f\"Input shape before LayerNorm: {x.shape}\")\n",
    "        # Residual connection and pre-normalization for attention\n",
    "        # Word embedding\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, hidden_size]\n",
    "        #print(\"token emb: \", x.shape)\n",
    "        # Positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        #print(\"Position: \", position_ids.shape)\n",
    "        x += self.position_embedding(position_ids)  # Add positional embeddings\n",
    "        #print( \"emb + pos\", x.shape)\n",
    "\n",
    "        residual = x\n",
    "        normed_input = self.input_layernorm(x)\n",
    "        #print(\"norme: \", normed_input.shape)\n",
    "\n",
    "        # Self-attention\n",
    "        attention_output = self.attention(normed_input, attention_mask)\n",
    "        #print(\"attention: \", attention_output.shape)\n",
    "        attention_output = self.dropout(attention_output) + residual\n",
    "\n",
    "        # Residual connection and pre-normalization for feed-forward\n",
    "        residual = attention_output\n",
    "        normed_attention_output = self.post_attention_layernorm(attention_output)\n",
    "\n",
    "        # Feed-forward network (Pattention)\n",
    "        mlp_output = self.mlp(normed_attention_output)\n",
    "        #print(\"feed: \",  mlp_output.shape)\n",
    "        output = self.dropout(mlp_output) + residual\n",
    "\n",
    "        # Linear layer for logits\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class remains unchanged\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, file_path, max_seq_len):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.text = f.read().lower().split()\n",
    "        \n",
    "        vocab = sorted(set(self.text))\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Convert text to token indices\n",
    "        self.tokens = [self.vocab[word] for word in self.text]\n",
    "        self.data = [\n",
    "            self.tokens[i : i + max_seq_len] for i in range(len(self.tokens) - max_seq_len)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_perplexity(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass through the model.\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "            y_batch = y_batch.view(-1)  # Flatten targets to shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss for the current batch.\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * y_batch.size(0)  # Multiply by the number of tokens.\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    # Calculate the average loss per token.\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # Compute the perplexity by exponentiating the average loss.\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "def train(use_metrics=False):\n",
    "    # Configurations\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # use M1 GPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # use CPU if no M1 GPU\n",
    "    hidden_dim=32 \n",
    "    num_heads=4\n",
    "    max_seq_len=32\n",
    "    batch_size=32\n",
    "    num_epochs=10\n",
    "    learning_rate=0.001\n",
    "    token_num=32\n",
    "    # vocab_size=10000\n",
    "    #num_layers=2,\n",
    "    #num_tokens=10,\n",
    "   \n",
    "\n",
    "    # Préparer les données\n",
    "    dataset = PokemonDataset(file_path='training/pokemon.txt', max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size  # Mettre à jour vocab_size après avoir créé le vocabulaire\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "    # Instancier le modèle\n",
    "    model = TokenformerLayer(hidden_dim, vocab_size, num_heads, max_seq_len,token_num=token_num)\n",
    "    model = model.to(device)\n",
    "    print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"number of training samples:\", len(dataset))\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"debut de l'entrainement\")\n",
    "    total_training_time = 0.0  # record the total training time\n",
    "    overall_start_time = time.time()  # record the overall start time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # record the start time of the current epoch\n",
    "        epoch_train_start = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move the batch data to the specified device.\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les logits pour la CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "\n",
    "            # Aplatir les labels\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # record the end time of the current epoch\n",
    "        epoch_training_time = time.time() - epoch_train_start\n",
    "        total_training_time += epoch_training_time\n",
    "\n",
    "        if use_metrics:\n",
    "            # record the start time of the evaluation\n",
    "            eval_start = time.time()\n",
    "            avg_loss, perplexity = estimate_perplexity(model, dataloader, criterion, device)\n",
    "            eval_time = time.time() - eval_start\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s, Eval Time: {eval_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Train Loss: {epoch_loss / len(dataloader):.4f}, \"\n",
    "                  f\"Train Time: {epoch_training_time:.2f}s\")\n",
    "\n",
    "    print(f\"Total training time (excluding evaluation): {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 10655\n",
      "model parameters: 703967\n",
      "number of training samples: 89205\n",
      "Total training tokens: 27.65M\n",
      "debut de l'entrainement\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 98\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(use_metrics)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Backpropagation et optimisation\u001b[39;00m\n\u001b[1;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    101\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(use_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Pattention Layer.\n",
    "    d1 = inputs dimension\n",
    "    d2 = outpuuts dimension\n",
    "    n = the number of parameters tokens representing the learnable keys and values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d1,\n",
    "        d2,\n",
    "        n,\n",
    "        param_key_init_method,\n",
    "        param_value_init_method,\n",
    "        norm_activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_token_num = n\n",
    "        self.param_key_dim = d1\n",
    "        self.param_value_dim = d2\n",
    "        self.norm_activation_type = norm_activation_type\n",
    "        \n",
    "        self.key_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d1))) # Kp shape of (n, d1) \n",
    "        self.value_param_tokens = nn.parameter.Parameter(data=torch.rand((n, d2))) # Vp shape of (n, d2)\n",
    "        \n",
    "        param_key_init_method(self.key_param_tokens)\n",
    "        param_value_init_method(self.value_param_tokens)\n",
    "\n",
    "    def add_new_params(self, num_new_tokens):\n",
    "        device = self.key_param_tokens.device  # Get the current device\n",
    "\n",
    "        new_key_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_key_dim), device=device))\n",
    "        new_value_tokens = nn.Parameter(torch.zeros((num_new_tokens, self.param_value_dim), device=device))\n",
    "\n",
    "        # Concatenate with existing parameters\n",
    "        self.key_param_tokens = nn.Parameter(torch.cat([self.key_param_tokens, new_key_tokens], dim=0))\n",
    "        self.value_param_tokens = nn.Parameter(torch.cat([self.value_param_tokens, new_value_tokens], dim=0))\n",
    "\n",
    "        #print(f\"Added {num_new_tokens} new tokens. Total tokens now: {self.key_param_tokens.shape[0]}\")\n",
    "    \n",
    "    def nonlinear_norm_func(self, inputs, normalize_type, dim=-1):\n",
    "        if normalize_type == 'softmax': \n",
    "            # NOTE: softmax = exp_l1_norm\n",
    "            # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "            nonlinear_outputs = torch.exp(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'gelu_l2_norm':\n",
    "            nonlinear_outputs = F.gelu(inputs)\n",
    "            norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "            outputs = norm_outputs\n",
    "        elif normalize_type == 'l2_norm_gelu':\n",
    "            norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "            nonlinear_outputs = F.gelu(norm_outputs)\n",
    "            outputs = nonlinear_outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, inputs, dropout_p=0.0, attn_mask=None, scale=None):\n",
    "\n",
    "        query = inputs\n",
    "        key, value = self.key_param_tokens, self.value_param_tokens        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        scale_factor = 1 if scale is None else scale \n",
    "        # just for gelu nonlinear, set torch.zeros for softmax\n",
    "        attn_bias = torch.ones(L, S, dtype=query.dtype, device=query.device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                # just for gelu nonlinear, set -inf for softmax\n",
    "                attn_bias.masked_fill_(attn_mask.logical_not(), 0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "        # just for gelu nonlinear, set attn_weight += attn_bias for softmax\n",
    "        attn_weight *= attn_bias\n",
    "        # modified softmax\n",
    "        attn_weight = self.nonlinear_norm_func(attn_weight, self.norm_activation_type, dim=-1)\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        output = attn_weight @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def progressive_train_and_scale(\n",
    "    file_path,\n",
    "    vocab_size,\n",
    "    max_seq_len,\n",
    "    hidden_dim,\n",
    "    num_heads,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    scaling_steps,\n",
    "    new_tokens_per_step,\n",
    "    num_tokens,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='checkpoints/tokenformer_base.pth',\n",
    "    use_metrics=False,\n",
    "    test_size=0.1,  # Default 10% of data as test set\n",
    "):\n",
    "    import time\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Assume PokemonDataset, TokenformerLayer, Pattention, save_model, load_model,\n",
    "    # count_parameters, and estimate_perplexity are defined or imported elsewhere.\n",
    "\n",
    "    # Set device based on availability (MPS for macOS or CPU)\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = PokemonDataset(file_path=file_path, max_seq_len=max_seq_len)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    \n",
    "    # Train-test split\n",
    "    train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model and move it to the appropriate device\n",
    "    model = TokenformerLayer(\n",
    "        hidden_size=hidden_dim,\n",
    "        vocab_size=vocab_size,\n",
    "        token_num=num_tokens,\n",
    "        num_attention_heads=num_heads,\n",
    "        max_seq_len=max_seq_len\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"number of training samples:\", len(dataset))\n",
    "    total_tokens = len(dataset) * (max_seq_len - 1) * num_epochs * (scaling_steps + 1)\n",
    "    print(f\"Total training tokens: {total_tokens/1e6:.2f}M\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Initial Training Phase\n",
    "    # ---------------------------\n",
    "    initial_start_time = time.time()\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch).view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if use_metrics:\n",
    "            avg_epoch_loss = total_loss / len(train_loader)\n",
    "            eval_loss, perplexity = estimate_perplexity(model, val_loader, criterion, device)\n",
    "            train_loss, _ = estimate_perplexity(model, train_loader, criterion, device)\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                  f\"Cumul Loss: {avg_epoch_loss:.4f}, Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "        else:\n",
    "            print(f\"Initial Training Epoch {epoch + 1}/{num_epochs}, Cumul Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    initial_end_time = time.time()\n",
    "    print(f\"Initial training time: {initial_end_time - initial_start_time:.2f} seconds\")\n",
    "    save_model(model, checkpoint_path)\n",
    "    print(f\"Initial model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    load_model(model, checkpoint_path)\n",
    "\n",
    "    # Dictionary to record training time for each scaling step (excluding evaluation time)\n",
    "    scaling_step_times = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Progressive Scaling and Fine-Tuning Phase\n",
    "    # ---------------------------\n",
    "    fine_tuning_start_time = time.time()\n",
    "    for step in range(scaling_steps):\n",
    "        # For each Pattention module in the model, add new parameters\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, Pattention):\n",
    "                module.add_new_params(new_tokens_per_step)\n",
    "        print(f\"Step {step + 1}: Model parameters after adding new params: {count_parameters(model)}\")\n",
    "\n",
    "        # Reinitialize the optimizer and save the current model parameters\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        checkpoint_path_step = f'checkpoints/tokenformer_scaled_step_{step + 1}.pth'\n",
    "        save_model(model, checkpoint_path_step)\n",
    "        print(f\"Step {step + 1}: Model parameters after scaling: {count_parameters(model)}\")\n",
    "        load_model(model, checkpoint_path_step)\n",
    "\n",
    "        # Record training time for the current scaling step (excluding evaluation)\n",
    "        step_training_time = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            # Start timing the training loop for this epoch\n",
    "            epoch_start_time = time.time()\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).view(-1, vocab_size)\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            # End timing the training loop for this epoch\n",
    "            epoch_end_time = time.time()\n",
    "            training_epoch_time = epoch_end_time - epoch_start_time\n",
    "            step_training_time += training_epoch_time\n",
    "\n",
    "            # If evaluation metrics are used, time them separately (but do not add to training time)\n",
    "            if use_metrics:\n",
    "                eval_start_time = time.time()\n",
    "                avg_epoch_loss = total_loss / len(train_loader)\n",
    "                eval_loss, perplexity = estimate_perplexity(model, val_loader, criterion, device)\n",
    "                train_loss, _ = estimate_perplexity(model, train_loader, criterion, device)\n",
    "                eval_end_time = time.time()\n",
    "                eval_time = eval_end_time - eval_start_time\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                      f\"Cumul Loss: {avg_epoch_loss:.4f}, Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f} \"\n",
    "                      f\"(Train Time: {training_epoch_time:.2f}s, Eval Time: {eval_time:.2f}s)\")\n",
    "            else:\n",
    "                print(f\"Step {step + 1}/{scaling_steps}, Fine-tune Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                      f\"Cumul Loss: {total_loss / len(train_loader):.4f} \"\n",
    "                      f\"(Train Time: {training_epoch_time:.2f}s)\")\n",
    "        \n",
    "        # Record and print the total training time for this scaling step (excluding evaluation)\n",
    "        scaling_step_times[step + 1] = step_training_time\n",
    "        print(f\"Step {step + 1} total training time (excluding evaluation): {step_training_time:.2f} seconds\")\n",
    "            \n",
    "    fine_tuning_end_time = time.time()\n",
    "    print(f\"Progressive scaling and fine-tuning total time (including evaluation): \"\n",
    "          f\"{fine_tuning_end_time - fine_tuning_start_time:.2f} seconds\")\n",
    "    print(f\"Overall training time (from initial training): \"\n",
    "          f\"{fine_tuning_end_time - initial_start_time:.2f} seconds\")\n",
    "    final_checkpoint_path = 'checkpoints/tokenformer_final_scaled.pth'\n",
    "    save_model(model, final_checkpoint_path)\n",
    "    print(f\"Final scaled model saved to {final_checkpoint_path}\")\n",
    "\n",
    "    # Optionally, print training times for each scaling step (excluding evaluation)\n",
    "    print(\"Per scaling step training times (excluding evaluation):\")\n",
    "    for step, t in scaling_step_times.items():\n",
    "        print(f\"  Step {step}: {t:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 698847\n",
      "number of training samples: 89205\n",
      "Total training tokens: 55.31M\n",
      "input_layernorm.weight: 32 parameters\n",
      "input_layernorm.bias: 32 parameters\n",
      "post_attention_layernorm.weight: 32 parameters\n",
      "post_attention_layernorm.bias: 32 parameters\n",
      "token_embedding.weight: 340960 parameters\n",
      "position_embedding.weight: 1024 parameters\n",
      "attention.query.key_param_tokens: 512 parameters\n",
      "attention.query.value_param_tokens: 512 parameters\n",
      "attention.key.key_param_tokens: 512 parameters\n",
      "attention.key.value_param_tokens: 512 parameters\n",
      "attention.value.key_param_tokens: 512 parameters\n",
      "attention.value.value_param_tokens: 512 parameters\n",
      "attention.out_proj.key_param_tokens: 512 parameters\n",
      "attention.out_proj.value_param_tokens: 512 parameters\n",
      "mlp.key_param_tokens: 512 parameters\n",
      "mlp.value_param_tokens: 512 parameters\n",
      "lm_head.weight: 340960 parameters\n",
      "lm_head.bias: 10655 parameters\n",
      "Starting initial training...\n"
     ]
    }
   ],
   "source": [
    "progressive_train_and_scale(\n",
    "    file_path='training/pokemon.txt',\n",
    "    vocab_size=10000,\n",
    "    max_seq_len=32,\n",
    "    hidden_dim=32,\n",
    "    num_heads=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    scaling_steps=3,\n",
    "    new_tokens_per_step=8,\n",
    "    num_tokens=16,\n",
    "    use_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
