{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: Config1: \n",
      "Total number of parameters: 1042061 (1.04 M)\n",
      "Iter 0: Loss = 4.4078, Perplexity = 82.0917\n",
      "Iter 100: Loss = 2.3900, Perplexity = 10.9129\n",
      "Iter 200: Loss = 2.0563, Perplexity = 7.8172\n",
      "Iter 300: Loss = 1.6735, Perplexity = 5.3309\n",
      "Iter 400: Loss = 1.4747, Perplexity = 4.3698\n",
      "Iter 500: Loss = 1.3685, Perplexity = 3.9295\n",
      "Iter 600: Loss = 1.2872, Perplexity = 3.6226\n",
      "Iter 700: Loss = 1.2309, Perplexity = 3.4245\n",
      "Iter 800: Loss = 1.1915, Perplexity = 3.2921\n",
      "Iter 900: Loss = 1.1564, Perplexity = 3.1785\n",
      "Iter 1000: Loss = 1.1255, Perplexity = 3.0816\n",
      "Iter 1100: Loss = 1.0954, Perplexity = 2.9905\n",
      "Iter 1200: Loss = 1.0687, Perplexity = 2.9116\n",
      "Iter 1300: Loss = 1.0428, Perplexity = 2.8372\n",
      "Iter 1400: Loss = 1.0178, Perplexity = 2.7670\n",
      "Iter 1500: Loss = 1.0047, Perplexity = 2.7311\n",
      "Iter 1600: Loss = 0.9822, Perplexity = 2.6703\n",
      "Iter 1700: Loss = 0.9601, Perplexity = 2.6119\n",
      "Iter 1800: Loss = 0.9448, Perplexity = 2.5722\n",
      "Iter 1900: Loss = 0.9231, Perplexity = 2.5171\n",
      "Iter 2000: Loss = 0.8993, Perplexity = 2.4579\n",
      "Iter 2100: Loss = 0.8838, Perplexity = 2.4202\n",
      "Iter 2200: Loss = 0.8613, Perplexity = 2.3661\n",
      "Iter 2300: Loss = 0.8441, Perplexity = 2.3259\n",
      "Iter 2400: Loss = 0.8266, Perplexity = 2.2854\n",
      "Iter 2500: Loss = 0.8143, Perplexity = 2.2576\n",
      "Iter 2600: Loss = 0.8032, Perplexity = 2.2326\n",
      "Iter 2700: Loss = 0.7857, Perplexity = 2.1940\n",
      "Iter 2800: Loss = 0.7678, Perplexity = 2.1551\n",
      "Iter 2900: Loss = 0.7518, Perplexity = 2.1209\n",
      "Iter 2999: Loss = 0.7400, Perplexity = 2.0959\n",
      "Config1: : Time = 121.22 sec, Final Loss = 0.7421, Perplexity = 2.1003\n",
      "\n",
      "Running experiment: Config2: \n",
      "Total number of parameters: 1589517 (1.59 M)\n",
      "Iter 0: Loss = 4.3445, Perplexity = 77.0516\n",
      "Iter 100: Loss = 2.3863, Perplexity = 10.8731\n",
      "Iter 200: Loss = 2.0220, Perplexity = 7.5533\n",
      "Iter 300: Loss = 1.6400, Perplexity = 5.1551\n",
      "Iter 400: Loss = 1.4502, Perplexity = 4.2639\n",
      "Iter 500: Loss = 1.3404, Perplexity = 3.8205\n",
      "Iter 600: Loss = 1.2661, Perplexity = 3.5470\n",
      "Iter 700: Loss = 1.2104, Perplexity = 3.3547\n",
      "Iter 800: Loss = 1.1591, Perplexity = 3.1871\n",
      "Iter 900: Loss = 1.1175, Perplexity = 3.0573\n",
      "Iter 1000: Loss = 1.0791, Perplexity = 2.9420\n",
      "Iter 1100: Loss = 1.0480, Perplexity = 2.8521\n",
      "Iter 1200: Loss = 1.0207, Perplexity = 2.7751\n",
      "Iter 1300: Loss = 0.9890, Perplexity = 2.6885\n",
      "Iter 1400: Loss = 0.9595, Perplexity = 2.6104\n",
      "Iter 1500: Loss = 0.9434, Perplexity = 2.5688\n",
      "Iter 1600: Loss = 0.9142, Perplexity = 2.4947\n",
      "Iter 1700: Loss = 0.8842, Perplexity = 2.4211\n",
      "Iter 1800: Loss = 0.8652, Perplexity = 2.3754\n",
      "Iter 1900: Loss = 0.8400, Perplexity = 2.3164\n",
      "Iter 2000: Loss = 0.8202, Perplexity = 2.2709\n",
      "Iter 2100: Loss = 0.7944, Perplexity = 2.2132\n",
      "Iter 2200: Loss = 0.7775, Perplexity = 2.1759\n",
      "Iter 2300: Loss = 0.7511, Perplexity = 2.1194\n",
      "Iter 2400: Loss = 0.7256, Perplexity = 2.0660\n",
      "Iter 2500: Loss = 0.7127, Perplexity = 2.0395\n",
      "Iter 2600: Loss = 0.6845, Perplexity = 1.9827\n",
      "Iter 2700: Loss = 0.6572, Perplexity = 1.9293\n",
      "Iter 2800: Loss = 0.6422, Perplexity = 1.9007\n",
      "Iter 2900: Loss = 0.6175, Perplexity = 1.8544\n",
      "Iter 2999: Loss = 0.6083, Perplexity = 1.8373\n",
      "Config2: : Time = 153.35 sec, Final Loss = 0.6078, Perplexity = 1.8364\n",
      "\n",
      "Running experiment: Config3: \n",
      "Total number of parameters: 2290893 (2.29 M)\n",
      "Iter 0: Loss = 4.3822, Perplexity = 80.0171\n",
      "Iter 100: Loss = 2.3851, Perplexity = 10.8602\n",
      "Iter 200: Loss = 1.9782, Perplexity = 7.2297\n",
      "Iter 300: Loss = 1.6117, Perplexity = 5.0114\n",
      "Iter 400: Loss = 1.4296, Perplexity = 4.1769\n",
      "Iter 500: Loss = 1.3156, Perplexity = 3.7270\n",
      "Iter 600: Loss = 1.2388, Perplexity = 3.4514\n",
      "Iter 700: Loss = 1.1777, Perplexity = 3.2469\n",
      "Iter 800: Loss = 1.1365, Perplexity = 3.1158\n",
      "Iter 900: Loss = 1.0890, Perplexity = 2.9712\n",
      "Iter 1000: Loss = 1.0497, Perplexity = 2.8569\n",
      "Iter 1100: Loss = 1.0153, Perplexity = 2.7603\n",
      "Iter 1200: Loss = 0.9801, Perplexity = 2.6647\n",
      "Iter 1300: Loss = 0.9509, Perplexity = 2.5881\n",
      "Iter 1400: Loss = 0.9148, Perplexity = 2.4963\n",
      "Iter 1500: Loss = 0.8879, Perplexity = 2.4301\n",
      "Iter 1600: Loss = 0.8520, Perplexity = 2.3444\n",
      "Iter 1700: Loss = 0.8256, Perplexity = 2.2832\n",
      "Iter 1800: Loss = 0.7967, Perplexity = 2.2183\n",
      "Iter 1900: Loss = 0.7612, Perplexity = 2.1409\n",
      "Iter 2000: Loss = 0.7309, Perplexity = 2.0769\n",
      "Iter 2100: Loss = 0.7030, Perplexity = 2.0197\n",
      "Iter 2200: Loss = 0.6772, Perplexity = 1.9684\n",
      "Iter 2300: Loss = 0.6552, Perplexity = 1.9254\n",
      "Iter 2400: Loss = 0.6286, Perplexity = 1.8750\n",
      "Iter 2500: Loss = 0.5971, Perplexity = 1.8169\n",
      "Iter 2600: Loss = 0.5731, Perplexity = 1.7737\n",
      "Iter 2700: Loss = 0.5528, Perplexity = 1.7381\n",
      "Iter 2800: Loss = 0.5239, Perplexity = 1.6885\n",
      "Iter 2900: Loss = 0.5101, Perplexity = 1.6654\n",
      "Iter 2999: Loss = 0.4787, Perplexity = 1.6139\n",
      "Config3: : Time = 190.37 sec, Final Loss = 0.4786, Perplexity = 1.6138\n",
      "\n",
      "Running experiment: Config4: \n",
      "Total number of parameters: 5595725 (5.60 M)\n",
      "Iter 0: Loss = 4.3787, Perplexity = 79.7350\n",
      "Iter 100: Loss = 2.4622, Perplexity = 11.7303\n",
      "Iter 200: Loss = 2.0627, Perplexity = 7.8669\n",
      "Iter 300: Loss = 1.7283, Perplexity = 5.6310\n",
      "Iter 400: Loss = 1.5166, Perplexity = 4.5565\n",
      "Iter 500: Loss = 1.3827, Perplexity = 3.9857\n",
      "Iter 600: Loss = 1.2810, Perplexity = 3.6004\n",
      "Iter 700: Loss = 1.1962, Perplexity = 3.3074\n",
      "Iter 800: Loss = 1.1375, Perplexity = 3.1189\n",
      "Iter 900: Loss = 1.0789, Perplexity = 2.9414\n",
      "Iter 1000: Loss = 1.0236, Perplexity = 2.7831\n",
      "Iter 1100: Loss = 0.9705, Perplexity = 2.6393\n",
      "Iter 1200: Loss = 0.9263, Perplexity = 2.5251\n",
      "Iter 1300: Loss = 0.8835, Perplexity = 2.4194\n",
      "Iter 1400: Loss = 0.8317, Perplexity = 2.2972\n",
      "Iter 1500: Loss = 0.7879, Perplexity = 2.1988\n",
      "Iter 1600: Loss = 0.7362, Perplexity = 2.0879\n",
      "Iter 1700: Loss = 0.6921, Perplexity = 1.9980\n",
      "Iter 1800: Loss = 0.6467, Perplexity = 1.9093\n",
      "Iter 1900: Loss = 0.6051, Perplexity = 1.8315\n",
      "Iter 2000: Loss = 0.5562, Perplexity = 1.7440\n",
      "Iter 2100: Loss = 0.5196, Perplexity = 1.6813\n",
      "Iter 2200: Loss = 0.4770, Perplexity = 1.6113\n",
      "Iter 2300: Loss = 0.4455, Perplexity = 1.5613\n",
      "Iter 2400: Loss = 0.4128, Perplexity = 1.5111\n",
      "Iter 2500: Loss = 0.3857, Perplexity = 1.4706\n",
      "Iter 2600: Loss = 0.3628, Perplexity = 1.4374\n",
      "Iter 2700: Loss = 0.3351, Perplexity = 1.3981\n",
      "Iter 2800: Loss = 0.3179, Perplexity = 1.3743\n",
      "Iter 2900: Loss = 0.3012, Perplexity = 1.3515\n",
      "Iter 2999: Loss = 0.2867, Perplexity = 1.3321\n",
      "Config4: : Time = 285.31 sec, Final Loss = 0.2877, Perplexity = 1.3334\n",
      "\n",
      "Running experiment: Config5: \n",
      "Total number of parameters: 14295629 (14.30 M)\n",
      "Iter 0: Loss = 4.4090, Perplexity = 82.1846\n",
      "Iter 100: Loss = 2.6069, Perplexity = 13.5568\n",
      "Iter 200: Loss = 2.4590, Perplexity = 11.6936\n",
      "Iter 300: Loss = 2.3964, Perplexity = 10.9838\n",
      "Iter 400: Loss = 2.1813, Perplexity = 8.8578\n",
      "Iter 500: Loss = 1.9602, Perplexity = 7.1006\n",
      "Iter 600: Loss = 1.8615, Perplexity = 6.4333\n",
      "Iter 700: Loss = 1.7876, Perplexity = 5.9753\n",
      "Iter 800: Loss = 1.7736, Perplexity = 5.8919\n",
      "Iter 900: Loss = 1.7444, Perplexity = 5.7223\n",
      "Iter 1000: Loss = 1.6991, Perplexity = 5.4691\n",
      "Iter 1100: Loss = 1.5810, Perplexity = 4.8598\n",
      "Iter 1200: Loss = 1.5623, Perplexity = 4.7699\n",
      "Iter 1300: Loss = 1.4808, Perplexity = 4.3965\n",
      "Iter 1400: Loss = 1.4431, Perplexity = 4.2340\n",
      "Iter 1500: Loss = 1.3714, Perplexity = 3.9408\n",
      "Iter 1600: Loss = 1.3063, Perplexity = 3.6924\n",
      "Iter 1700: Loss = 1.2674, Perplexity = 3.5515\n",
      "Iter 1800: Loss = 1.2263, Perplexity = 3.4087\n",
      "Iter 1900: Loss = 1.1934, Perplexity = 3.2984\n",
      "Iter 2000: Loss = 1.1610, Perplexity = 3.1931\n",
      "Iter 2100: Loss = 1.1326, Perplexity = 3.1037\n",
      "Iter 2200: Loss = 1.0998, Perplexity = 3.0035\n",
      "Iter 2300: Loss = 1.0755, Perplexity = 2.9314\n",
      "Iter 2400: Loss = 1.0382, Perplexity = 2.8241\n",
      "Iter 2500: Loss = 1.0031, Perplexity = 2.7268\n",
      "Iter 2600: Loss = 0.9718, Perplexity = 2.6428\n",
      "Iter 2700: Loss = 0.9435, Perplexity = 2.5690\n",
      "Iter 2800: Loss = 0.9100, Perplexity = 2.4844\n",
      "Iter 2900: Loss = 0.8792, Perplexity = 2.4090\n",
      "Iter 2999: Loss = 0.8450, Perplexity = 2.3280\n",
      "Config5: : Time = 539.83 sec, Final Loss = 0.8458, Perplexity = 2.3298\n",
      "\n",
      "All experiments results:\n",
      "Config1: : Time = 121.22 sec, Loss = 0.7421, Perplexity = 2.1003\n",
      "Config2: : Time = 153.35 sec, Loss = 0.6078, Perplexity = 1.8364\n",
      "Config3: : Time = 190.37 sec, Loss = 0.4786, Perplexity = 1.6138\n",
      "Config4: : Time = 285.31 sec, Loss = 0.2877, Perplexity = 1.3334\n",
      "Config5: : Time = 539.83 sec, Loss = 0.8458, Perplexity = 2.3298\n",
      "\n",
      "Results saved to experiment_results.csv\n",
      "\n",
      "Generated text:\n",
      "\n",
      "\n",
      "\n",
      "\"What you serituation that Charmander!\" The Pokéball cried out, Pidgeotto snapped open and smacked tired into his good glow descely in the area. Something the few energefightibles bened Alto Mare. Both had occelice energy to reach the beautiful Electriever-Type. Me Pokémon closed another Badge. She had glanced the scept of his hand understood its feet, Lucario answered by the suggestion of coursed through the dark glow straight throught their in such as he answered to be grabbed in anger as th\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "\n",
    "# -------------------------\n",
    "# General hyperparameters (initial values)\n",
    "# -------------------------\n",
    "batch_size = 64          # Number of sequences per batch\n",
    "block_size = 128         # Maximum sequence length\n",
    "max_iters = 3000         # Total number of iterations\n",
    "eval_interval = 100      # Evaluate loss every this many iterations\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200         # Number of batches to use when computing the average loss during evaluation\n",
    "n_head = 4               # Number of heads in multi-head attention\n",
    "dropout = 0              # Dropout probability (set to 0 here)\n",
    "# n_embd and n_layer will be adjusted in different configurations\n",
    "\n",
    "# -------------------------\n",
    "# Device configuration (MPS or CPU)\n",
    "# -------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# -------------------------\n",
    "# Data preprocessing\n",
    "# -------------------------\n",
    "with open('pokemon.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    dat = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(dat) - block_size, (batch_size,))\n",
    "    x = torch.stack([dat[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([dat[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_and_ppl(model):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch('train')\n",
    "        _, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    avg_loss = losses.mean().item()\n",
    "    model.train()\n",
    "    return {\"train\": avg_loss}, {\"train\": math.exp(avg_loss)}\n",
    "\n",
    "# -------------------------\n",
    "# Model definition section\n",
    "# -------------------------\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Single self-attention head \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head attention module \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Feed-forward network \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)        # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.blocks(x)     # (B, T, n_embd)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# -------------------------\n",
    "# Experiment section: Models with different configurations\n",
    "# -------------------------\n",
    "# We define several configurations here by adjusting n_embd and n_layer,\n",
    "# which causes the parameter count to increase rapidly.\n",
    "configs = [\n",
    "    {\"name\": \"Config1: \", \"n_layer\": 4, \"n_embd\": 144},\n",
    "    {\"name\": \"Config2: \", \"n_layer\": 5, \"n_embd\": 160},\n",
    "    {\"name\": \"Config3: \", \"n_layer\": 6, \"n_embd\": 176},\n",
    "    {\"name\": \"Config4: \", \"n_layer\": 7, \"n_embd\": 256},\n",
    "    {\"name\": \"Config5: \", \"n_layer\": 8, \"n_embd\": 384},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nRunning experiment: {config['name']}\")\n",
    "    # Update global variables: Note that the global variables n_layer and n_embd\n",
    "    # are used during model construction.\n",
    "    n_layer = config[\"n_layer\"]\n",
    "    n_embd = config[\"n_embd\"]\n",
    "    \n",
    "    # Reconstruct the model (the model constructor uses the current n_embd, n_layer, etc.)\n",
    "    model = GPTLanguageModel().to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params} ({total_params/1e6:.2f} M)\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses, perplexities = estimate_loss_and_ppl(model)\n",
    "            print(f\"Iter {iter}: Loss = {losses['train']:.4f}, Perplexity = {perplexities['train']:.4f}\")\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    losses, perplexities = estimate_loss_and_ppl(model)\n",
    "    exp_result = {\n",
    "        \"Config\": config[\"name\"],\n",
    "        \"n_layer\": config[\"n_layer\"],\n",
    "        \"n_embd\": config[\"n_embd\"],\n",
    "        \"Max_iters\": max_iters,\n",
    "        \"ParameterCount\": total_params,\n",
    "        \"FinalLoss\": losses[\"train\"],\n",
    "        \"FinalPerplexity\": perplexities[\"train\"],\n",
    "        \"TrainingTime\": total_time\n",
    "    }\n",
    "    results.append(exp_result)\n",
    "    print(f\"{config['name']}: Time = {total_time:.2f} sec, Final Loss = {losses['train']:.4f}, Perplexity = {perplexities['train']:.4f}\")\n",
    "\n",
    "print(\"\\nAll experiments results:\")\n",
    "for res in results:\n",
    "    print(f\"{res['Config']}: Time = {res['TrainingTime']:.2f} sec, Loss = {res['FinalLoss']:.4f}, Perplexity = {res['FinalPerplexity']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save experiment results to a CSV file\n",
    "# -------------------------\n",
    "csv_filename = \"experiment_results.csv\"\n",
    "fieldnames = [\"Config\", \"n_layer\", \"n_embd\", \"Max_iters\", \"ParameterCount\", \"FinalLoss\", \"FinalPerplexity\", \"TrainingTime\"]\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for res in results:\n",
    "        writer.writerow(res)\n",
    "\n",
    "print(f\"\\nResults saved to {csv_filename}\")\n",
    "\n",
    "# -------------------------\n",
    "# Generate sample text using the last trained model\n",
    "# -------------------------\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial token 0\n",
    "generated_idx = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(\"\\nGenerated text:\")\n",
    "print(decode(generated_idx))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
